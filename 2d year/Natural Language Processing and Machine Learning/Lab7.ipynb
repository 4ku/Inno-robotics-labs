{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1yR4nzdCpkJ"
   },
   "source": [
    "# Lab 7: Finishing ngrams and Word sense disambiguation with Lesk algorithm\n",
    "\n",
    "```\n",
    "Plan : \n",
    "  1. Ngrams as features, Skip-grams\n",
    "  1. Simplified Lesk algorithm, using WordNet\n",
    "  1. (optional) WSD with Naive Bayes Classifier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSvAiDr_ljzz"
   },
   "source": [
    "## Ngrams and skip-grams as features\n",
    "\n",
    "What limitations of Bag-of-Words model do you remember?\n",
    "\n",
    "One of them is a loss of word order. For the following ewo sentences the BoW are identical, however, the labels are different.\n",
    "* No, this movie is good. [ Positive sentiment]\n",
    "* This movie is no good. [ Negative sentiment]\n",
    "\n",
    "So, introducing Ngrams into the Bag-of-Words model (Bag-of-Ngrams) can mitigate this limitation (How?). What else can be done to improve?\n",
    "\n",
    "* This movie is not quite good.\n",
    "* This movie is not that good.\n",
    "* This movie is not very good.\n",
    "\n",
    "Can you see a pattern here? It seems we could add \"not _ good\" instead of all the possible ngrams. Such ngrams are called Skip-grams.\n",
    "\n",
    "**N-grams** are sequences of adjacent units (letters, words, or whatever counting unit you happen to care about) of length \"n\"; it's a cover term for bigrams (sequences of 2 adjacent things, n = 2), trigrams (sequences of three adjacent things), 4-grams, etc.\n",
    "\n",
    "\n",
    "**Skip-grams** (or \"k-skip-n-grams\") are sequences of ordered but not-necessarily-adjacent (thus \"skipped\") units, where the gaps can be at most \"k\" units long.\n",
    "\n",
    "\n",
    "For example, in the sentence \"The quick brown fox jumped over the lazy dog\"\n",
    "* bigrams (2-grams) include \"The quick\", \"quick brown\", \"brown fox\", fox jumped\", \"jumped over\", \"over the\", \"the lazy\", and \"lazy dog\",\n",
    "* 1-skip-2-grams include all of the bigrams in addition to \"the _ brown\", \"quick _ fox\", \"brown _ jumped\", \"fox _ over\", \"jumped _ the\", \"over _ lazy\", and \"the _ dog\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze1OZjZxsLyH"
   },
   "source": [
    "### Implement a TfIdf Classifier, which will take unigrams, bigram and 1-skip 2-grams as input. Compare it with a similar classifier with only unigrams input.\n",
    "\n",
    "Possible dataset: [IMDB movie reviews](https://huggingface.co/datasets/imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ivan/miniconda3/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ivan/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.23.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ivan/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ivan/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.9.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /home/ivan/miniconda3/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/ros/noetic/lib/python3/dist-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ivan/.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pandas in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /home/ivan/miniconda3/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ivan/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ivan/miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /home/ivan/miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ivan/miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/ros/noetic/lib/python3/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/ros/noetic/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t_YNUi4Vsk0Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ivan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd3776f8a2f44e2b1e98b97a67c47f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your code here \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "train_texts = train_data[\"text\"]\n",
    "train_labels = train_data[\"label\"]\n",
    "test_texts = test_data[\"text\"]\n",
    "test_labels = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_skip_bigrams(tokens):\n",
    "    skip_bigrams = set()\n",
    "    for i in range(len(tokens) - 2):\n",
    "        skip_bigrams.add((tokens[i], tokens[i + 2]))\n",
    "    return list(skip_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    unigrams = tokens\n",
    "    bigrams = [\"_\".join(bigram) for bigram in zip(tokens[:-1], tokens[1:])]\n",
    "    one_skip_bigrams = [\"_\".join(skip_bigram) for skip_bigram in generate_one_skip_bigrams(tokens)]\n",
    "    return unigrams + bigrams + one_skip_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bigram_skip_bigram_vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=custom_tokenizer, token_pattern=None)\n",
    "unigram_vectorizer = TfidfVectorizer(ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ubs = unigram_bigram_skip_bigram_vectorizer.fit_transform(train_texts)\n",
    "X_test_ubs = unigram_bigram_skip_bigram_vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unigram = unigram_vectorizer.fit_transform(train_texts)\n",
    "X_test_unigram = unigram_vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_ubs = LogisticRegression(max_iter=1000)\n",
    "classifier_ubs.fit(X_train_ubs, train_labels)\n",
    "\n",
    "classifier_unigram = LogisticRegression(max_iter=1000)\n",
    "classifier_unigram.fit(X_train_unigram, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram + Bigram + 1-skip 2-gram Classifier Accuracy: 85.90%\n",
      "Unigram Classifier Accuracy: 88.32%\n"
     ]
    }
   ],
   "source": [
    "y_pred_ubs = classifier_ubs.predict(X_test_ubs)\n",
    "accuracy_ubs = accuracy_score(test_labels, y_pred_ubs)\n",
    "print(\"Unigram + Bigram + 1-skip 2-gram Classifier Accuracy: {:.2f}%\".format(accuracy_ubs * 100))\n",
    "\n",
    "y_pred_unigram = classifier_unigram.predict(X_test_unigram)\n",
    "accuracy_unigram = accuracy_score(test_labels, y_pred_unigram)\n",
    "print(\"Unigram Classifier Accuracy: {:.2f}%\".format(accuracy_unigram * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zch1ZReFCnGT"
   },
   "source": [
    "## Simplified Lesk algorithm, using WordNet.\n",
    "\n",
    "NLTK provides an interface to access and explore WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlfk5Xy2C0rl",
    "outputId": "71375a42-d616-4726-88ad-0692906db2f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgnMWbvBpQkC"
   },
   "source": [
    "We can find all synsets that contain a given word's base form, or lemma, using `lemmas(word)`\n",
    "\n",
    "* To match a word, it must be given in its base form \n",
    "\n",
    "Words may be converted automatically to their base forms using **`morphy(word)`**, which takes the word and an optional part-of-speech as arguments, and returns the base form of the given word with the matching PoS (or any, if none given)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ETE9yanpolb"
   },
   "source": [
    "### Lists of synsets and their definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8d8kdyTEUMh",
    "outputId": "6a63cd28-2f57-41f3-e6fb-5323f61ff966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('interest.n.01.interest')\n",
      "a sense of concern with and curiosity about someone or something\n",
      "Lemma('sake.n.01.interest')\n",
      "a reason for wanting something done\n",
      "Lemma('interest.n.03.interest')\n",
      "the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
      "Lemma('interest.n.04.interest')\n",
      "a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
      "Lemma('interest.n.05.interest')\n",
      "(law) a right or legal share of something; a financial involvement with something\n",
      "Lemma('interest.n.06.interest')\n",
      "(usually plural) a social group whose members control some field of activity and who have common aims\n",
      "Lemma('pastime.n.01.interest')\n",
      "a diversion that occupies one's time and thoughts (usually pleasantly)\n",
      "Lemma('interest.v.01.interest')\n",
      "excite the curiosity of; engage the interest of\n",
      "Lemma('concern.v.02.interest')\n",
      "be on the mind of\n",
      "Lemma('matter_to.v.01.interest')\n",
      "be of importance or consequence\n"
     ]
    }
   ],
   "source": [
    "w = wordnet.morphy('interest')\n",
    "for l in wordnet.lemmas(w):\n",
    "  print(l)\n",
    "  print(l.synset().definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thXDPlKSqI3h"
   },
   "source": [
    "## Simplified Lesk\n",
    "\n",
    "The Simplified Lesk chooses the word sense which has the most in common between: its dictionary definition and examples, and the context of the target word.\n",
    "\n",
    "For example, if we are interested in identifying the correct sense of the word **`interest`** in the following context:\n",
    "\n",
    "```\n",
    "While some in the United States cheered the election victory of Democrat Barack Obama, on the other side of the world, \n",
    "Chinese showed concern and interest over the state of the economy.\n",
    "```\n",
    "\n",
    "Choose sense with best matches <br>\n",
    "**NB**: ignore stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jn1XKc3oeDUt"
   },
   "outputs": [],
   "source": [
    "context = set(['concern', 'over', 'state','economy'])\n",
    "w_of_int = 'interest'\n",
    "\n",
    "sysnsets = wordnet.synsets(w_of_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "4_Ij5ve4jOVt",
    "outputId": "47fba0fd-92eb-4f94-de34-4276f5123075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('interest.n.06')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'excite the curiosity of; engage the interest of'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sent = 'While some in the United States cheered the election victory of Democrat Barack Obama, on the other side of the world, Chinese showed concern and interest over the state of the economy.'\n",
    "ambiguous = 'interest'\n",
    "print(lesk(sent.split(), ambiguous,'n'))\n",
    "lesk(sent.split(), ambiguous).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HQxVaUqFGhS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('interest.n.01')\n",
      "a sense of concern with and curiosity about someone or something\n"
     ]
    }
   ],
   "source": [
    "## TODO : Write your code here and test it with the example sentence above\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def simplified_lesk(sentence, ambiguous_word, pos='n', stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    context = set(word_tokenize(sentence)) - stopwords\n",
    "    synsets = wordnet.synsets(ambiguous_word, pos)\n",
    "\n",
    "    best_sense = None\n",
    "    max_overlap = -1\n",
    "\n",
    "    for sense in synsets:\n",
    "        # Get the set of words in the definition and examples of the sense\n",
    "        sense_words = set(word_tokenize(sense.definition())) | set(word_tokenize(' '.join(sense.examples())))\n",
    "        sense_words -= stopwords\n",
    "\n",
    "        # Calculate the overlap with the context\n",
    "        overlap = len(context & sense_words)\n",
    "\n",
    "        # Update the best_sense if overlap is larger\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense\n",
    "\n",
    "# Test with the example sentence\n",
    "sentence = 'While some in the United States cheered the election victory of Democrat Barack Obama, on the other side of the world, Chinese showed concern and interest over the state of the economy.'\n",
    "ambiguous_word = 'interest'\n",
    "\n",
    "# Get the best sense and its definition\n",
    "best_sense = simplified_lesk(sentence, ambiguous_word)\n",
    "print(best_sense)\n",
    "print(best_sense.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X12DDlbyxMtF"
   },
   "source": [
    "## Task\n",
    "\n",
    "```\n",
    "1. Run your algorithm over the 300 Senseval example sentences.\n",
    "\n",
    "2. Test accuracy using Senseval-2 corpus\n",
    " - Can be installed in NLTK The tags are a little different:\n",
    "      HARD1 corresponds to difficult.a.01 \n",
    "      HARD2 corresponds to hard.a.02 \n",
    "      HARD3 corresponds to hard.a.03.hard\n",
    "  - but the order is the same\n",
    "\n",
    "* Remember to exclude stop words\n",
    "\n",
    "3. Does this algorithm perform better than simply selecting the most common sense for all examples?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnDAdKOE6Hbl"
   },
   "source": [
    "### Download Senseval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJYpQbBDyaY5",
    "outputId": "e3d70dbd-8bd0-4958-9e2f-6fa7d8cba699"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import senseval as se\n",
    "# nltk.download('senseval')\n",
    "se.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3an4-dK4mwC",
    "outputId": "67647859-1dd5-4e07-e966-93f1c3198e39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SensevalInstance(word='hard-a', position=20, context=[('``', '``'), ('he', 'PRP'), ('may', 'MD'), ('lose', 'VB'), ('all', 'DT'), ('popular', 'JJ'), ('support', 'NN'), (',', ','), ('but', 'CC'), ('someone', 'NN'), ('has', 'VBZ'), ('to', 'TO'), ('kill', 'VB'), ('him', 'PRP'), ('to', 'TO'), ('defeat', 'VB'), ('him', 'PRP'), ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('do', 'VB'), ('.', '.'), (\"''\", \"''\")], senses=('HARD1',)),\n",
       " SensevalInstance(word='hard-a', position=10, context=[('clever', 'NNP'), ('white', 'NNP'), ('house', 'NNP'), ('``', '``'), ('spin', 'VB'), ('doctors', 'NNS'), (\"''\", \"''\"), ('are', 'VBP'), ('having', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('time', 'NN'), ('helping', 'VBG'), ('president', 'NNP'), ('bush', 'NNP'), ('explain', 'VB'), ('away', 'RB'), ('the', 'DT'), ('economic', 'JJ'), ('bashing', 'NN'), ('that', 'IN'), ('low-and', 'JJ'), ('middle-income', 'JJ'), ('workers', 'NNS'), ('are', 'VBP'), ('taking', 'VBG'), ('these', 'DT'), ('days', 'NNS'), ('.', '.')], senses=('HARD1',)),\n",
       " SensevalInstance(word='hard-a', position=3, context=[('i', 'PRP'), ('find', 'VBP'), ('it', 'PRP'), ('hard', 'JJ'), ('to', 'TO'), ('believe', 'VB'), ('that', 'IN'), ('the', 'DT'), ('sacramento', 'NNP'), ('river', 'NNP'), ('will', 'MD'), ('ever', 'RB'), ('be', 'VB'), ('quite', 'RB'), ('the', 'DT'), ('same', 'JJ'), (',', ','), ('although', 'IN'), ('i', 'PRP'), ('certainly', 'RB'), ('wish', 'VBP'), ('that', 'IN'), ('i', 'PRP'), (\"'m\", 'VBP'), ('wrong', 'JJ'), ('.', '.')], senses=('HARD1',)),\n",
       " SensevalInstance(word='hard-a', position=15, context=[('now', 'RB'), ('when', 'WRB'), ('you', 'PRP'), ('get', 'VBP'), ('bad', 'JJ'), ('credit', 'NN'), ('data', 'NNS'), ('or', 'CC'), ('are', 'VBP'), ('confused', 'VBN'), ('with', 'IN'), ('another', 'DT'), ('person', 'NN'), (',', ','), ('the', 'DT'), ('hard', 'JJ'), ('part', 'NN'), ('in', 'IN'), ('correcting', 'VBG'), ('the', 'DT'), ('mistake', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('even', 'RB'), ('knowing', 'VBG'), ('where', 'WRB'), ('it', 'PRP'), ('is', 'VBZ'), ('recorded', 'VBN'), (',', ','), ('let', 'VB'), ('alone', 'RB'), ('having', 'VBG'), ('access', 'NN'), ('.', '.')], senses=('HARD1',)),\n",
       " SensevalInstance(word='hard-a', position=66, context=[(\"'a\", 'NN'), ('great', 'JJ'), ('share', 'NN'), ('of', 'IN'), ('responsibility', 'NN'), ('for', 'IN'), ('this', 'DT'), ('national', 'JJ'), ('tragedy', 'NN'), ('unquestionably', 'RB'), ('lies', 'VBZ'), ('with', 'IN'), ('the', 'DT'), ('president', 'NN'), ('of', 'IN'), ('the', 'DT'), ('country', 'NN'), ('.', '.'), (\"'\", \"''\"), ('--', ':'), ('eduard', 'NNP'), ('shevardnadze', 'NNP'), (',', ','), ('former', 'JJ'), ('foreign', 'JJ'), ('minister', 'NN'), (';', ':'), (\"'we\", 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('deep', 'JJ'), ('in', 'IN'), ('this', 'DT'), ('crisis', 'NN'), ('that', 'IN'), ('all', 'PDT'), ('this', 'DT'), ('business', 'NN'), ('about', 'IN'), ('leaving', 'VBG'), ('the', 'DT'), ('party', 'NN'), (',', ','), ('not', 'RB'), ('leaving', 'VBG'), ('the', 'DT'), ('party', 'NN'), ('--', ':'), ('that', 'WDT'), ('will', 'MD'), ('never', 'RB'), ('get', 'VB'), ('us', 'PRP'), ('out', 'IN'), ('.', '.'), (\"'\", \"''\"), ('--', ':'), ('natasha', 'NNP'), (',', ','), ('a', 'DT'), ('moscow', 'NNP'), ('bookkeeper', 'NN'), (';', ':'), (\"'our\", 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('harder', 'JJ'), ('now', 'RB'), (',', ','), ('yes', 'UH'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('better', 'JJR'), ('to', 'TO'), ('be', 'VB'), ('hungry', 'JJ'), ('and', 'CC'), ('free', 'JJ'), ('.', '.'), (\"'\", \"''\"), ('--', ':'), ('lena', 'NNP'), ('sedykh', 'NNP'), (',', ','), ('a', 'DT'), ('moscow', 'NNP'), ('street', 'NN'), ('sweeper', 'NN'), (';', ':'), (\"'if\", 'NN'), ('you', 'PRP'), ('judge', 'VBP'), ('by', 'IN'), ('astrology', 'NN'), (',', ','), ('gorbachev', 'NNP'), ('is', 'VBZ'), ('cancer', 'NNP'), (',', ','), ('and', 'CC'), ('yeltsin', 'NNP'), ('and', 'CC'), ('russia', 'NNP'), ('are', 'VBP'), ('aquarius', 'NNP'), ('.', '.')], senses=('HARD1',))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from Senseval\n",
    "se.instances()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0miAEP7Q3lyR",
    "outputId": "ecded76f-c052-4e3c-d7c6-2303894f3e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('difficult.a.01.hard')\n",
      "not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
      "Lemma('hard.a.02.hard')\n",
      "dispassionate\n",
      "Lemma('hard.a.03.hard')\n",
      "resisting weight or pressure\n",
      "Lemma('hard.s.04.hard')\n",
      "very strong or vigorous\n",
      "Lemma('arduous.s.01.hard')\n",
      "characterized by effort to the point of exhaustion; especially physical effort\n",
      "Lemma('unvoiced.a.01.hard')\n",
      "produced without vibration of the vocal cords\n",
      "Lemma('hard.a.07.hard')\n",
      "(of light) transmitted directly from a pointed light source\n",
      "Lemma('hard.a.08.hard')\n",
      "(of speech sounds); produced with the back of the tongue raised toward or touching the velum\n",
      "Lemma('intemperate.s.03.hard')\n",
      "given to excessive indulgence of bodily appetites especially for intoxicating liquors\n",
      "Lemma('hard.s.10.hard')\n",
      "being distilled rather than fermented; having a high alcoholic content\n",
      "Lemma('hard.s.11.hard')\n",
      "unfortunate or hard to bear\n",
      "Lemma('hard.s.12.hard')\n",
      "dried out\n",
      "Lemma('hard.r.01.hard')\n",
      "with effort or force or vigor\n",
      "Lemma('hard.r.02.hard')\n",
      "with firmness\n",
      "Lemma('hard.r.03.hard')\n",
      "earnestly or intently\n",
      "Lemma('hard.r.04.hard')\n",
      "causing great damage or hardship\n",
      "Lemma('hard.r.05.hard')\n",
      "slowly and with difficulty\n",
      "Lemma('heavily.r.07.hard')\n",
      "indulging excessively\n",
      "Lemma('hard.r.07.hard')\n",
      "into a solid condition\n",
      "Lemma('hard.r.08.hard')\n",
      "very near or close in space or time\n",
      "Lemma('hard.r.09.hard')\n",
      "with pain or distress or bitterness\n",
      "Lemma('hard.r.10.hard')\n",
      "to the full extent possible; all the way\n"
     ]
    }
   ],
   "source": [
    "w = wordnet.morphy('hard')\n",
    "for l in wordnet.lemmas(w):\n",
    "  print(l)\n",
    "  print(l.synset().definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JARKD_v49T5",
    "outputId": "6733da6b-7900-4859-944a-5ce1859ad9bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('serve.n.01.serve')\n",
      "(sports) a stroke that puts the ball in play\n",
      "Lemma('serve.v.01.serve')\n",
      "serve a purpose, role, or function\n",
      "Lemma('serve.v.02.serve')\n",
      "do duty or hold offices; serve in a specific function\n",
      "Lemma('serve.v.03.serve')\n",
      "contribute or conduce to\n",
      "Lemma('service.v.01.serve')\n",
      "be used by; as of a utility\n",
      "Lemma('serve.v.05.serve')\n",
      "help to some food; help with food or drink\n",
      "Lemma('serve.v.06.serve')\n",
      "provide (usually but not necessarily food)\n",
      "Lemma('serve.v.07.serve')\n",
      "devote (part of) one's life or efforts to, as of countries, institutions, or ideas\n",
      "Lemma('serve.v.08.serve')\n",
      "promote, benefit, or be useful or beneficial to\n",
      "Lemma('serve.v.09.serve')\n",
      "spend time in prison or in a labor camp\n",
      "Lemma('serve.v.10.serve')\n",
      "work for or be a servant to\n",
      "Lemma('serve.v.11.serve')\n",
      "deliver a warrant or summons to someone\n",
      "Lemma('suffice.v.01.serve')\n",
      "be sufficient; be adequate, either in quality or quantity\n",
      "Lemma('serve.v.13.serve')\n",
      "do military service\n",
      "Lemma('serve.v.14.serve')\n",
      "mate with\n",
      "Lemma('serve.v.15.serve')\n",
      "put the ball into play\n"
     ]
    }
   ],
   "source": [
    "w = wordnet.morphy('serve')\n",
    "for l in wordnet.lemmas(w):\n",
    "  print(l)\n",
    "  print(l.synset().definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and test Simplified Lesk on Senseval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6171\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def map_senseval_to_wordnet(sense):\n",
    "    # Extract numbers\n",
    "    numbers = re.findall(r'\\d+', sense)\n",
    "    number = int(numbers[0])\n",
    "\n",
    "    # Extract word\n",
    "    word = re.findall(r'\\D+', sense)\n",
    "    word = word[0].lower() \n",
    "    \n",
    "    lemma = wordnet.lemmas(word)[number-1]\n",
    "    return lemma.synset().name()\n",
    "\n",
    "def test_simplified_lesk_accuracy():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for instance in se.instances('hard.pos'):\n",
    "        context_sentence = ' '.join(element[0] for element in instance.context if isinstance(element, tuple))\n",
    "\n",
    "        ambiguous_word, pos = instance.word.split('-')\n",
    "        correct_sense = instance.senses[0]\n",
    "        correct_sense = map_senseval_to_wordnet(correct_sense)\n",
    "\n",
    "        predicted_sense = simplified_lesk(context_sentence, ambiguous_word, pos)\n",
    "\n",
    "        if predicted_sense is not None and predicted_sense.name() == correct_sense:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = test_simplified_lesk_accuracy()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the most common sense accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Sense Accuracy: 0.7974\n"
     ]
    }
   ],
   "source": [
    "def most_common_sense_accuracy():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for instance in se.instances('hard.pos'):\n",
    "            ambiguous_word, pos = instance.word.split('-')\n",
    "            correct_sense = instance.senses[0]\n",
    "            correct_sense = map_senseval_to_wordnet(correct_sense)\n",
    "            \n",
    "            # Get the most common sense for the ambiguous word\n",
    "            synsets = wordnet.synsets(ambiguous_word, pos)\n",
    "            most_common_sense = synsets[0].name() if synsets else None\n",
    "\n",
    "            if most_common_sense is not None and most_common_sense == correct_sense:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Calculate the most common sense accuracy\n",
    "most_common_sense_accuracy = most_common_sense_accuracy()\n",
    "print(f\"Most Common Sense Accuracy: {most_common_sense_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simplified Lesk algorithm does not perform better than simply selecting the most common sense for all examples in this case. The most common sense selection method has a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTR5OoENC1ys"
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Creating text features with bag-of-words, n-grams, parts-of-speach and more](https://uc-r.github.io/creating-text-features)\n",
    "1. [Speech and Language Processing. Daniel Jurafsky & James H. Martin.](https://web.stanford.edu/~jurafsky/slp3/18.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTq3-vsnH-eN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
