{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gau9xEXMGY8s"
   },
   "source": [
    "# Neural Machine Translation with Attention Using PyTorch\n",
    "In this notebook we are going to perform machine translation using a deep learning based approach and attention mechanism. All code is based on PyTorch and it was adopted from the tutorial provided on the official documentation of [TensorFlow](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb).\n",
    "\n",
    "Specifically, we are going to train a sequence to sequence model for French-to-English translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z579-ISl9Zj6"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qT20LFmb3jSW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/ivan/miniconda3/lib/python3.8/site-packages (4.7.1)\n",
      "Requirement already satisfied: tqdm in /home/ivan/.local/lib/python3.8/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ivan/miniconda3/lib/python3.8/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: filelock in /home/ivan/miniconda3/lib/python3.8/site-packages (from gdown) (3.8.0)\n",
      "Requirement already satisfied: six in /opt/ros/noetic/lib/python3/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /home/ivan/miniconda3/lib/python3.8/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ivan/miniconda3/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ivan/miniconda3/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sh0N1AxpXhGY",
    "outputId": "1cd0c404-1a2a-47e9-a094-5346aac1e1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Bhpi9gD_3UHZRFmcn7Czkb73jHN8CUAL\n",
      "To: /home/ivan/Downloads/Dataset-fr-eng.txt\n",
      "100%|██████████████████████████████████████| 28.7M/28.7M [00:09<00:00, 2.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1Bhpi9gD_3UHZRFmcn7Czkb73jHN8CUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JD8Qy0eC0ZtA"
   },
   "outputs": [],
   "source": [
    "f = open('Dataset-fr-eng.txt', encoding='UTF-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0mVlB0W14b4G"
   },
   "outputs": [],
   "source": [
    "lines = f\n",
    "# sample size (smaller sample size to reduce computation)\n",
    "num_examples = 30000 \n",
    "# creates lists containing each pair\n",
    "original_word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"fr\",\"whatever\"])\n",
    "data = data[['eng','fr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "913VSLih4lY3",
    "outputId": "19c55147-a932-445e-8357-5bc4f772ceaa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Prenez vos jambes à vos cous !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run!</td>\n",
       "      <td>File !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Filez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Fuyez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Fuyons !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Prenez vos jambes à vos cous !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Run.</td>\n",
       "      <td>File !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Filez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Fuyez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Fuyons !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Duck!</td>\n",
       "      <td>À terre !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Duck!</td>\n",
       "      <td>Baisse-toi !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Duck!</td>\n",
       "      <td>Baissez-vous !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Help!</td>\n",
       "      <td>À l'aide !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hide.</td>\n",
       "      <td>Cache-toi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hide.</td>\n",
       "      <td>Cachez-vous.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Jump!</td>\n",
       "      <td>Saute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>Saute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Ça suffit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Stop !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Arrête-toi !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>Attends !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>Attendez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>Attendez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>Attends !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>Attendez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>Attends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>Attendez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Begin.</td>\n",
       "      <td>Commencez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Begin.</td>\n",
       "      <td>Commence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Go on.</td>\n",
       "      <td>Poursuis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Go on.</td>\n",
       "      <td>Continuez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Go on.</td>\n",
       "      <td>Poursuivez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>Bonjour !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>I see.</td>\n",
       "      <td>Je comprends.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eng                              fr\n",
       "0      Go.                            Va !\n",
       "1      Go.                         Marche.\n",
       "2      Go.                         Bouge !\n",
       "3      Hi.                         Salut !\n",
       "4      Hi.                          Salut.\n",
       "5     Run!                         Cours !\n",
       "6     Run!                        Courez !\n",
       "7     Run!  Prenez vos jambes à vos cous !\n",
       "8     Run!                          File !\n",
       "9     Run!                         Filez !\n",
       "10    Run!                         Cours !\n",
       "11    Run!                         Fuyez !\n",
       "12    Run!                        Fuyons !\n",
       "13    Run.                         Cours !\n",
       "14    Run.                        Courez !\n",
       "15    Run.  Prenez vos jambes à vos cous !\n",
       "16    Run.                          File !\n",
       "17    Run.                         Filez !\n",
       "18    Run.                         Cours !\n",
       "19    Run.                         Fuyez !\n",
       "20    Run.                        Fuyons !\n",
       "21    Who?                           Qui ?\n",
       "22    Wow!                      Ça alors !\n",
       "23   Duck!                       À terre !\n",
       "24   Duck!                    Baisse-toi !\n",
       "25   Duck!                  Baissez-vous !\n",
       "26   Fire!                        Au feu !\n",
       "27   Help!                      À l'aide !\n",
       "28   Hide.                      Cache-toi.\n",
       "29   Hide.                    Cachez-vous.\n",
       "30   Jump!                          Saute.\n",
       "31   Jump.                          Saute.\n",
       "32   Stop!                     Ça suffit !\n",
       "33   Stop!                          Stop !\n",
       "34   Stop!                    Arrête-toi !\n",
       "35   Wait!                       Attends !\n",
       "36   Wait!                      Attendez !\n",
       "37   Wait!                       Attendez.\n",
       "38   Wait.                       Attends !\n",
       "39   Wait.                      Attendez !\n",
       "40   Wait.                        Attends.\n",
       "41   Wait.                       Attendez.\n",
       "42  Begin.                      Commencez.\n",
       "43  Begin.                       Commence.\n",
       "44  Go on.                       Poursuis.\n",
       "45  Go on.                      Continuez.\n",
       "46  Go on.                     Poursuivez.\n",
       "47  Hello!                       Bonjour !\n",
       "48  Hello!                         Salut !\n",
       "49  I see.                   Je comprends."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jCUSf31E4m6t"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    \"\"\"\n",
    "    Normalizes latin chars with accent to their canonical decomposition\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN2pLaZkNqrv"
   },
   "source": [
    "## Data Exploration\n",
    "Let's explore the dataset a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "QFLV4RCR4pXa",
    "outputId": "37a94356-b384-4f36-b354-fbf86f30ab90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;start&gt; run . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; courez ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14653</th>\n",
       "      <td>&lt;start&gt; it s cold today . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; il fait froid aujourd hui . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6581</th>\n",
       "      <td>&lt;start&gt; do you see it ? &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; est ce que vous le voyez ? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13421</th>\n",
       "      <td>&lt;start&gt; how big you are ! &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; comme tu es grand ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>&lt;start&gt; he is lazy . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; il est faineant . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>&lt;start&gt; are you dressed ? &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; etes vous habille ? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>&lt;start&gt; tom has a job . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; tom travaille . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>&lt;start&gt; i m pregnant . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; je suis enceinte . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19385</th>\n",
       "      <td>&lt;start&gt; my cat is hungry . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; mon chat a faim . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26226</th>\n",
       "      <td>&lt;start&gt; we can build that . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; nous pouvons construire cela . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     eng  \\\n",
       "14                   <start> run . <end>   \n",
       "14653    <start> it s cold today . <end>   \n",
       "6581       <start> do you see it ? <end>   \n",
       "13421    <start> how big you are ! <end>   \n",
       "1803          <start> he is lazy . <end>   \n",
       "12676    <start> are you dressed ? <end>   \n",
       "8533       <start> tom has a job . <end>   \n",
       "5153        <start> i m pregnant . <end>   \n",
       "19385   <start> my cat is hungry . <end>   \n",
       "26226  <start> we can build that . <end>   \n",
       "\n",
       "                                                 fr  \n",
       "14                           <start> courez ! <end>  \n",
       "14653     <start> il fait froid aujourd hui . <end>  \n",
       "6581       <start> est ce que vous le voyez ? <end>  \n",
       "13421             <start> comme tu es grand ! <end>  \n",
       "1803                <start> il est faineant . <end>  \n",
       "12676             <start> etes vous habille ? <end>  \n",
       "8533                  <start> tom travaille . <end>  \n",
       "5153               <start> je suis enceinte . <end>  \n",
       "19385               <start> mon chat a faim . <end>  \n",
       "26226  <start> nous pouvons construire cela . <end>  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we do the preprocessing using pandas and lambdas\n",
    "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
    "data[\"fr\"] = data.fr.apply(lambda w: preprocess_sentence(w))\n",
    "data.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqM7ZncM8V9B"
   },
   "source": [
    "#### Building Vocabulary Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rXA7-N34sok",
    "outputId": "05d92fa6-c633-4ca4-909e-856d815f1ef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 7355, 1, 4],\n",
       " [5, 4357, 3, 4],\n",
       " [5, 905, 1, 4],\n",
       " [5, 6432, 1, 4],\n",
       " [5, 6432, 3, 4],\n",
       " [5, 1663, 1, 4],\n",
       " [5, 1655, 1, 4],\n",
       " [5, 5559, 7581, 3985, 7, 7581, 1677, 1, 4],\n",
       " [5, 3125, 1, 4],\n",
       " [5, 3127, 1, 4]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            # update with individual tokens\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        # sort the vocab\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        # add a padding token with index 0\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        # word to index mapping\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
    "        # index to word mapping\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word      \n",
    "\n",
    "\n",
    "# index language using the class above\n",
    "inp_lang = LanguageIndex(data[\"fr\"].values.tolist())\n",
    "targ_lang = LanguageIndex(data[\"eng\"].values.tolist())\n",
    "# Vectorize the input and target languages\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in es.split(' ')]  for es in data[\"fr\"].values.tolist()]\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]\n",
    "input_tensor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fesymsn34v7z",
    "outputId": "a659c0aa-8bb7-4e09-d638-d6ff94d62e22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1588, 3, 4],\n",
       " [5, 1588, 3, 4],\n",
       " [5, 1588, 3, 4],\n",
       " [5, 1774, 3, 4],\n",
       " [5, 1774, 3, 4],\n",
       " [5, 3188, 1, 4],\n",
       " [5, 3188, 1, 4],\n",
       " [5, 3188, 1, 4],\n",
       " [5, 3188, 1, 4],\n",
       " [5, 3188, 1, 4]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8cwX-0rt4zmN"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# calculate the max_length of input and output tensor\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "def pad_sequences(x, max_len):\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len: padded[:] = x[:max_len]\n",
    "    else: padded[:len(x)] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66dJPqzV44jd",
    "outputId": "9ed28b04-5bb2-4a77-90e1-a707c78729e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inplace padding\n",
    "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
    "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
    "len(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvatfCWS46T-",
    "outputId": "65c6c2a8-1ee4-4a1a-ec39-0f63890ece0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNFO3obpOsoB"
   },
   "source": [
    "## Load data into DataLoader for Batching\n",
    "This is just preparing the dataset so that it can be efficiently fed into the model through batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-QRQKwxf479Q"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IDSxA4OM5Qlp"
   },
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x,y,x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2WukeVF8NVn"
   },
   "source": [
    "## Parameters\n",
    "Let's define the hyperparameters and other things we need for training our NMT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s3Be7lOZ5R-d"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
    "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
    "\n",
    "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
    "                     drop_last=True,\n",
    "                     shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, \n",
    "                     drop_last=True,\n",
    "                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "blYXo7pv5TOu"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
    "        \n",
    "    def forward(self, x, device):\n",
    "        # x: batch_size, max_length \n",
    "        \n",
    "        # x: batch_size, max_length, embedding_dim\n",
    "        x = self.embedding(x)\n",
    "    \n",
    "        self.hidden = self.initialize_hidden_state(device)\n",
    "        \n",
    "        # output: max_length, batch_size, enc_units\n",
    "        # self.hidden: 1, batch_size, enc_units\n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "               \n",
    "        return output, self.hidden\n",
    "\n",
    "    def initialize_hidden_state(self, device):\n",
    "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiMRxHQFGPtt"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input word is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "  \n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "t4djvgil5bMQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
    "                          self.dec_units,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.V = nn.Linear(self.enc_units, 1)\n",
    "    \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
    "        enc_output = enc_output.permute(1,0,2)\n",
    "      \n",
    "        # hidden shape == (batch_size, hidden size) we convert it to (batch_size, 1, hidden size)\n",
    "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
    "        \n",
    "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
    "        # It doesn't matter which FC we pick for each of the inputs\n",
    "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # calculate attention weights using softmax\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = F.softmax(self.V(score), dim=1)\n",
    "        \n",
    "        # calculate context_vector\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = torch.sum(attention_weights * enc_output, axis=1)\n",
    "\n",
    "        # pass the context vector into embedding embedding layer\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        ##  concatenate the context vector and x\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output =  output.view(-1, output.size(2))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return torch.zeros((1, self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QclyWIop5dRG"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LjMMYJv85hVT"
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                       lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6_WoDZM7reU"
   },
   "source": [
    "## Training\n",
    "Now we start the training. We are only using 10 epochs but you can expand this to keep trainining the model for a longer period of time. Note that in this case we are teacher forcing during training. Find a more detailed explanation in the official TensorFlow [implementation](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) of this notebook provided by the TensorFlow team. \n",
    "\n",
    "- Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "- The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "- The decoder returns the predictions and the decoder hidden state.\n",
    "- The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "- Use teacher forcing to decide the next input to the decoder.\n",
    "- Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "- The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2QUJ5YDYmDp"
   },
   "source": [
    "# Why do we need mask in loss function?\n",
    "\n",
    "### *Your answer:* \n",
    "The mask is needed in the loss function because the model is trained to predict the entire sequence, but we only want to consider the non-zero inputs when calculating the loss. In other words, we do not want to penalize the model for predicting zeros when the actual target is also a zero. The mask is used to ignore the zeros in the loss calculation.\n",
    "\n",
    "# When do we apply teacher forcing - during training and/or testing?\n",
    "\n",
    "### *Your answer:* \n",
    "Teacher forcing is used only during training, not during testing. During training, we use the ground truth target sequence as the input to the decoder at each time step, rather than using the decoder's own output from the previous time step as the input. This is done to ensure that the decoder is trained to predict the correct next word in the sequence, rather than just memorizing the previous predictions. However, during testing, we use the decoder's own output from the previous time step as the input to predict the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BpQU6MhEUfFL"
   },
   "outputs": [],
   "source": [
    "def flip_batch(X, y):\n",
    "    return X.transpose(0,1), y # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KN8G-3YY8ADm",
    "outputId": "82d4775d-cf4d-4ba4-8d32-0034d7567da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6236\n",
      "Epoch 1 Batch 100 Loss 1.5164\n",
      "Epoch 1 Batch 200 Loss 1.3514\n",
      "Epoch 1 Batch 300 Loss 1.1261\n",
      "Epoch 2 Batch 0 Loss 0.8623\n",
      "Epoch 2 Batch 100 Loss 0.8067\n",
      "Epoch 2 Batch 200 Loss 0.6598\n",
      "Epoch 2 Batch 300 Loss 0.6423\n",
      "Epoch 3 Batch 0 Loss 0.3595\n",
      "Epoch 3 Batch 100 Loss 0.3352\n",
      "Epoch 3 Batch 200 Loss 0.3628\n",
      "Epoch 3 Batch 300 Loss 0.3891\n",
      "Epoch 4 Batch 0 Loss 0.2362\n",
      "Epoch 4 Batch 100 Loss 0.1997\n",
      "Epoch 4 Batch 200 Loss 0.2005\n",
      "Epoch 4 Batch 300 Loss 0.2358\n",
      "Epoch 5 Batch 0 Loss 0.1134\n",
      "Epoch 5 Batch 100 Loss 0.1354\n",
      "Epoch 5 Batch 200 Loss 0.1193\n",
      "Epoch 5 Batch 300 Loss 0.0957\n",
      "Epoch 6 Batch 0 Loss 0.0774\n",
      "Epoch 6 Batch 100 Loss 0.0759\n",
      "Epoch 6 Batch 200 Loss 0.0892\n",
      "Epoch 6 Batch 300 Loss 0.0985\n",
      "Epoch 7 Batch 0 Loss 0.0447\n",
      "Epoch 7 Batch 100 Loss 0.0990\n",
      "Epoch 7 Batch 200 Loss 0.0479\n",
      "Epoch 7 Batch 300 Loss 0.0995\n",
      "Epoch 8 Batch 0 Loss 0.0593\n",
      "Epoch 8 Batch 100 Loss 0.0409\n",
      "Epoch 8 Batch 200 Loss 0.0910\n",
      "Epoch 8 Batch 300 Loss 0.1094\n",
      "Epoch 9 Batch 0 Loss 0.0797\n",
      "Epoch 9 Batch 100 Loss 0.0704\n",
      "Epoch 9 Batch 200 Loss 0.0783\n",
      "Epoch 9 Batch 300 Loss 0.0968\n",
      "Epoch 10 Batch 0 Loss 0.0513\n",
      "Epoch 10 Batch 100 Loss 0.0897\n",
      "Epoch 10 Batch 200 Loss 0.0517\n",
      "Epoch 10 Batch 300 Loss 0.0962\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "encoder.batch_sz = 64\n",
    "encoder.initialize_hidden_state(device)\n",
    "decoder.batch_sz = 64\n",
    "decoder.initialize_hidden_state()\n",
    "\n",
    "for epoch in range(EPOCHS):    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        xs, ys = flip_batch(inp, targ)\n",
    "        enc_output, enc_hidden = encoder(xs.to(device), device)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
    "        for t in range(1, ys.size(1)):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
    "                                         dec_hidden.to(device), \n",
    "                                         enc_output.to(device))\n",
    "            \n",
    "            loss += loss_function(ys[:, t].long().to(device), predictions.to(device))\n",
    "            dec_input = ys[:, t].unsqueeze(1)\n",
    "\n",
    "        batch_loss = (loss / int(ys.size(1)))\n",
    "        total_loss += batch_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.detach().item()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vYBffn-2Hao",
    "outputId": "9e2a3dfc-251b-41a9-88e1-1b3353a09221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'love',\n",
       " 'egg',\n",
       " '.',\n",
       " '<end>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_sentence(encoder, decoder, sentence, max_length=120):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    sentence = torch.unsqueeze(sentence, dim=1)\n",
    "    with torch.no_grad():\n",
    "        enc_output, enc_hidden = encoder(sentence.to(device), device)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)\n",
    "        out_sentence = []\n",
    "        for t in range(1, sentence.size(0)):  \n",
    "            \n",
    "            # why there is a loop?\n",
    "            # answer:\n",
    "\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
    "                                        dec_hidden.to(device), \n",
    "                                        enc_output.to(device))\n",
    "            dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
    "            out_sentence.append(targ_lang.idx2word[predictions.squeeze().argmax().item()])\n",
    "\n",
    "    return out_sentence\n",
    "\n",
    "encoder.batch_sz = 1\n",
    "encoder.initialize_hidden_state(device)\n",
    "decoder.batch_sz = 1\n",
    "decoder.initialize_hidden_state()\n",
    "\n",
    "test_sentence = \"<start> j adore les fleurs . <end>\"\n",
    "test_sentence = [inp_lang.word2idx[s] for s in test_sentence.split(' ')]\n",
    "test_sentence = pad_sequences(test_sentence, max_length_inp)\n",
    "ret = translate_sentence(encoder, decoder, torch.tensor(test_sentence), max_length=120)\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcd2vJINHj1"
   },
   "source": [
    "### References\n",
    "\n",
    "\n",
    "\n",
    "1.   http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/\n",
    "2.   https://medium.com/dair-ai/neural-machine-translation-with-attention-using-pytorch-a66523f1669f\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
