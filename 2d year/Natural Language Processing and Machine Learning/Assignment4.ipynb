{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z5Jca5Ah2b8"
   },
   "source": [
    "# POS tagging\n",
    "\n",
    "During labs, we have covered HMM, LSTM and BERT models, the goal of this assignment is to evaluate and compare these models on POS tagging task. The input is a text line and the outputs are POS tags for every word (token) in the input line.\n",
    "\n",
    "- You should already have the code for the models from labs\n",
    "- Use validation split to decide when to stop training\n",
    "- Evaluate all models on test data\n",
    "- You can use any PoS tagging dataset to train and test your models\n",
    "\n",
    "Refer to:\n",
    "- Lab 4 - HMM for Tagging\n",
    "- Lab 10 - LSTM for Tagging\n",
    "- Lab 5 - Hugging Face and BERT fine-tuning\n",
    "- [Datasets](https://universaldependencies.org/)\n",
    "- [Dataset from Labs 4 and 10](https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt)\n",
    "\n",
    "\n",
    "Grading:\n",
    "- 30 points - HMM\n",
    "- 30 points - BiLSTM\n",
    "- 30 points - BERT (for masters only)\n",
    "- 40 points - Evaluation and conclusions \n",
    "\n",
    "\n",
    "Remarks: \n",
    "- Use Python 3\n",
    "- Max is 100 points for bachelors, 130 points for masters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o7l16MVh2cH"
   },
   "source": [
    "# Hidden Markov Models for POS Tagging\n",
    "\n",
    "You can use the Viterbi algorithm implementation from Lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyO7uizYqqBX",
    "outputId": "2bc69234-4c85-482b-8e6f-768d3630e017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-23 15:10:52--  https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1855828 (1.8M) [text/plain]\n",
      "Saving to: ‘train_pos.txt.2’\n",
      "\n",
      "\r",
      "train_pos.txt.2       0%[                    ]       0  --.-KB/s               \r",
      "train_pos.txt.2     100%[===================>]   1.77M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2023-04-23 15:10:52 (31.5 MB/s) - ‘train_pos.txt.2’ saved [1855828/1855828]\n",
      "\n",
      "--2023-04-23 15:10:52--  https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 418682 (409K) [text/plain]\n",
      "Saving to: ‘test_pos.txt.2’\n",
      "\n",
      "test_pos.txt.2      100%[===================>] 408.87K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-04-23 15:10:52 (11.0 MB/s) - ‘test_pos.txt.2’ saved [418682/418682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
    "!wget https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jl1fFNRtTJFS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi(y, A, B, Pi=None):\n",
    "    \"\"\"\n",
    "    Return the MAP estimate of state trajectory of Hidden Markov Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array (T,)\n",
    "        Observation state sequence. int dtype.\n",
    "    A : array (K, K)\n",
    "        State transition matrix. See HiddenMarkovModel.state_transition  for\n",
    "        details.\n",
    "    B : array (K, M)\n",
    "        Emission matrix. See HiddenMarkovModel.emission for details.\n",
    "    Pi: optional, (K,)\n",
    "        Initial state probabilities: Pi[i] is the probability x[0] == i. If\n",
    "        None, uniform initial distribution is assumed (Pi[:] == 1/K).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : array (T,)\n",
    "        Maximum a posteriori probability estimate of hidden state trajectory,\n",
    "        conditioned on observation sequence y under the model parameters A, B,\n",
    "        Pi.\n",
    "    T1: array (K, T)\n",
    "        the probability of the most likely path so far\n",
    "    T2: array (K, T)\n",
    "        the x_j-1 of the most likely path so far\n",
    "    \"\"\"\n",
    "    # Cardinality of the state space\n",
    "    K = A.shape[0]\n",
    "    # Initialize the priors with default (uniform dist) if not given by caller\n",
    "    Pi = Pi if Pi is not None else np.full(K, 1 / K)\n",
    "    T = len(y)\n",
    "    T1 = np.empty((K, T), 'd')\n",
    "    T2 = np.empty((K, T), 'B')\n",
    "\n",
    "    # Initilaize the tracking tables from first observation\n",
    "    T1[:, 0] = Pi * B[:, y[0]]\n",
    "    T2[:, 0] = 0\n",
    "\n",
    "    # Iterate throught the observations updating the tracking tables\n",
    "    for i in range(1, T):\n",
    "        T1[:, i] = np.max(T1[:, i - 1] * A.T * B[np.newaxis, :, y[i]].T, 1)\n",
    "        T2[:, i] = np.argmax(T1[:, i - 1] * A.T, 1)\n",
    "\n",
    "    # Build the output, optimal model trajectory\n",
    "    x = np.empty(T, 'B')\n",
    "    x[-1] = np.argmax(T1[:, T - 1])\n",
    "    for i in reversed(range(1, T)):\n",
    "        x[i - 1] = T2[x[i], i]\n",
    "\n",
    "    return x, T1, T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HYBxJPEJTJFT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_with_pos(dataset):\n",
    "    tokens_with_pos = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in dataset:\n",
    "        if len(line.split()) == 2:\n",
    "            word, tag = line.split()\n",
    "            word = word.lower()\n",
    "            sentence.append((word, tag))\n",
    "        elif len(line.split()) == 0:\n",
    "            tokens_with_pos.append(sentence)\n",
    "            sentence = []\n",
    "    return tokens_with_pos\n",
    "\n",
    "# download the dataset https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
    "# read the dataset\n",
    "with open(\"train_pos.txt\", \"r\") as file:\n",
    "    dataset = file.readlines()\n",
    "\n",
    "# tokenize the dataset preserving the PoS tags information\n",
    "tokenized_dataset = tokenize_with_pos(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z4-0pIM3TJFV"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "# extract the following statistics A, B, Pi from the data:\n",
    "# A - State transition matrix, the probabilities of a PoS Tag2 occuring after PoS Tag1 (matrix N_tags x N_tags)\n",
    "# B - Emission matrix, the probabilities of a word corresponding to the given Pos Tag (matrix N_tags x N_words)\n",
    "# Pi - Initial state probabilities, the probabilities of a tag starting a sentence (vector N_tags) \n",
    "\n",
    "# Count the occurrences of each tag\n",
    "tag_counts = Counter()\n",
    "for sentence in tokenized_dataset:\n",
    "    for token in sentence:\n",
    "        tag = token[1]\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Create a list of all possible tags\n",
    "all_tags = list(tag_counts.keys())\n",
    "\n",
    "# Count the occurrences of each tag pair\n",
    "tag_pair_counts = Counter()\n",
    "for sentence in tokenized_dataset:\n",
    "    for pair in zip_longest(sentence, sentence[1:], fillvalue=(\"\", \".\")):\n",
    "        tag1, tag2 = pair[0][1], pair[1][1]\n",
    "        tag_pair_counts[(tag1, tag2)] += 1\n",
    "\n",
    "# Create the state transition matrix A\n",
    "A = np.zeros((len(all_tags), len(all_tags)))\n",
    "for i, tag1 in enumerate(all_tags):\n",
    "    for j, tag2 in enumerate(all_tags):\n",
    "        A[i, j] = tag_pair_counts[(tag1, tag2)] / tag_counts[tag1]\n",
    "\n",
    "# Count the occurrences of each word for each tag\n",
    "word_tag_counts = {tag: Counter() for tag in all_tags}\n",
    "for sentence in tokenized_dataset:\n",
    "    for token in sentence:\n",
    "        word, tag = token\n",
    "        word_tag_counts[tag][word] += 1\n",
    "\n",
    "# Create the emission matrix B\n",
    "all_words = set()\n",
    "for tag in all_tags:\n",
    "    all_words.update(word_tag_counts[tag].keys())\n",
    "all_words = list(all_words)\n",
    "\n",
    "B = np.zeros((len(all_tags), len(all_words)))\n",
    "for i, tag in enumerate(all_tags):\n",
    "    for j, word in enumerate(all_words):\n",
    "        B[i, j] = word_tag_counts[tag][word] / tag_counts[tag]\n",
    "\n",
    "# Create the initial state probabilities vector Pi\n",
    "initial_tags = [sentence[0][1] for sentence in tokenized_dataset]\n",
    "initial_tag_counts = Counter(initial_tags)\n",
    "Pi = np.zeros(len(all_tags))\n",
    "for i, tag in enumerate(all_tags):\n",
    "    Pi[i] = initial_tag_counts[tag] / len(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pSfRkFITJFV",
    "outputId": "fb3e0b89-61eb-4bd0-be3c-c1f9d181df8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Sequence: ['PRP', 'VBP', 'VBN', 'TO', 'VB', 'IN', 'NN', 'JJ', 'NN']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I am excited to go on vacation next week\"\n",
    "word_indices = [all_words.index(word.lower()) for word in sentence1.split()]\n",
    "tags_indices, T1, T2 = viterbi(word_indices, A, B, Pi)\n",
    "print(\"POS Sequence:\", [all_tags[x] for x in tags_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NpmDBW-TJFW"
   },
   "source": [
    "## Evaluate HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgZekHN8TJFW",
    "outputId": "bb9fcf45-82d0-4bdd-b20c-bda2759ed2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.864596745256137\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read and tokenize the test dataset\n",
    "with open(\"test_pos.txt\", \"r\") as file:\n",
    "    test_dataset = file.readlines()\n",
    "\n",
    "test_tokenized_dataset = tokenize_with_pos(test_dataset)\n",
    "\n",
    "# Step 2: Predict the POS tags for each sentence in the test dataset\n",
    "predicted_tags = []\n",
    "true_tags = []\n",
    "\n",
    "for sentence in test_tokenized_dataset:\n",
    "    words = [token[0] for token in sentence]\n",
    "    true_tags_sentence = [token[1] for token in sentence]\n",
    "    \n",
    "    word_indices = [all_words.index(word) if word in all_words else -1 for word in words]\n",
    "    \n",
    "    known_word_indices = [i for i in word_indices if i != -1]\n",
    "    \n",
    "    if known_word_indices:  # Only predict if there are known words in the sentence\n",
    "        tags_indices, _, _ = viterbi(known_word_indices, A, B, Pi)\n",
    "        predicted_tags_sentence = [all_tags[x] for x in tags_indices]\n",
    "    else:\n",
    "        predicted_tags_sentence = []\n",
    "\n",
    "    # Insert default tags for unseen words\n",
    "    for i, index in enumerate(word_indices):\n",
    "        if index == -1:\n",
    "            predicted_tags_sentence.insert(i, \"None\")\n",
    "    \n",
    "    predicted_tags.extend(predicted_tags_sentence)\n",
    "    true_tags.extend(true_tags_sentence)\n",
    "\n",
    "# Step 3: Compare the predicted POS tags with the ground truth POS tags\n",
    "correct_count = sum(p == t for p, t in zip(predicted_tags, true_tags))\n",
    "total_count = len(true_tags)\n",
    "\n",
    "# Step 4: Calculate evaluation metrics such as accuracy\n",
    "accuracy = correct_count / total_count\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLRfCS58h2cJ"
   },
   "source": [
    "# LSTM for POS Tagging\n",
    "\n",
    "Use a 2-layer BiLSTM from pytorch as we did in Lab 10\n",
    "- nn.LSTM(..., num_layers=2, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2y-Cxq8ITL5M",
    "outputId": "e8366855-c9c2-419f-ff97-56f9cbb6f1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-23 15:11:38--  https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1855828 (1.8M) [text/plain]\n",
      "Saving to: ‘train_pos.txt.3’\n",
      "\n",
      "train_pos.txt.3     100%[===================>]   1.77M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2023-04-23 15:11:38 (32.2 MB/s) - ‘train_pos.txt.3’ saved [1855828/1855828]\n",
      "\n",
      "--2023-04-23 15:11:38--  https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 418682 (409K) [text/plain]\n",
      "Saving to: ‘test_pos.txt.3’\n",
      "\n",
      "test_pos.txt.3      100%[===================>] 408.87K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-04-23 15:11:39 (11.3 MB/s) - ‘test_pos.txt.3’ saved [418682/418682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
    "! wget https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yrRsIfMSTJFZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class PoSDataset(Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sentences[index], self.tags[index]\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    sentences_raw = content.split(\" \\n\")[:-1]\n",
    "    sentences, tags = [], []\n",
    "    for sent in sentences_raw:\n",
    "        words, pos_tags = [], []\n",
    "        for word_tag in sent.split(\"\\n\"):\n",
    "            if word_tag:\n",
    "                word, tag = word_tag.split(\" \")\n",
    "                words.append(word)\n",
    "                pos_tags.append(tag)\n",
    "        sentences.append(words)\n",
    "        tags.append(pos_tags)\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RSesx2WETJFZ"
   },
   "outputs": [],
   "source": [
    "train_sentences, train_tags = read_data(\"train_pos.txt\")\n",
    "test_sentences, test_tags = read_data(\"test_pos.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xDWnlNQ8TJFa"
   },
   "outputs": [],
   "source": [
    "def create_vocab(sentences, tags):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    tags_vocab = {\"<PAD>\": 0}\n",
    "\n",
    "    for sent, pos_tags in zip(sentences, tags):\n",
    "        for word, tag in zip(sent, pos_tags):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            if tag not in tags_vocab:\n",
    "                tags_vocab[tag] = len(tags_vocab)\n",
    "\n",
    "    return vocab, tags_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dw1aT2rnTJFa"
   },
   "outputs": [],
   "source": [
    "vocab, tags_vocab = create_vocab(train_sentences, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N-9fqljXTJFa"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad(sentences, tags, vocab, tags_vocab, seq_len):\n",
    "    tokenized_sentences = []\n",
    "    tokenized_tags = []\n",
    "\n",
    "    for sent, pos_tags in zip(sentences, tags):\n",
    "        tokenized_sent = [vocab.get(word, vocab[\"<UNK>\"]) for word in sent[:seq_len]]\n",
    "        tokenized_tag = [tags_vocab[tag] for tag in pos_tags[:seq_len]]\n",
    "\n",
    "        # Padding\n",
    "        tokenized_sent += [vocab[\"<PAD>\"]] * (seq_len - len(tokenized_sent))\n",
    "        tokenized_tag += [tags_vocab[\"<PAD>\"]] * (seq_len - len(tokenized_tag))\n",
    "\n",
    "        tokenized_sentences.append(tokenized_sent)\n",
    "        tokenized_tags.append(tokenized_tag)\n",
    "\n",
    "    return tokenized_sentences, tokenized_tags\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_input, batch_output = [], []\n",
    "    for x in batch:\n",
    "        batch_input.append(x[0])\n",
    "        batch_output.append(x[1])\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.int)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Nk76n4cVTJFb"
   },
   "outputs": [],
   "source": [
    "# Find the longest sentence in the dataset\n",
    "seq_len = max([len(sent) for sent in train_sentences])\n",
    "SEQ_LEN = seq_len\n",
    "\n",
    "train_sentences, train_tags = tokenize_and_pad(train_sentences, train_tags, vocab, tags_vocab, seq_len)\n",
    "test_sentences, test_tags = tokenize_and_pad(test_sentences, test_tags, vocab, tags_vocab, seq_len)\n",
    "\n",
    "train_dataset = PoSDataset(train_sentences, train_tags)\n",
    "test_dataset = PoSDataset(test_sentences, test_tags)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r9Wq7SfBTJFc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(2 * hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(sentence.shape[0], sentence.shape[1], -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(sentence.shape[0] * sentence.shape[1], -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5PQY-FWLTJFc"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "VOCAB_SIZE = len(vocab)\n",
    "TARGET_SIZE = len(tags_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HylIIAUYTJFd"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "\n",
    "# define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# make model instance and send it to training device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FB7FW_qATJFd"
   },
   "outputs": [],
   "source": [
    "def accuracy_calculator(preds, y):\n",
    "    return (preds == y).sum() / len(y)\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for text, tags in dataloader:\n",
    "        text = text.to(device)\n",
    "        tags = tags.to(device)\n",
    "\n",
    "        # initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predict tags and compute loss\n",
    "        predictions = model(text).view(-1, TARGET_SIZE)\n",
    "        loss = criterion(predictions, tags.view(-1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(predictions, dim=1)\n",
    "        acc = accuracy_calculator(preds, tags.view(-1))\n",
    "        \n",
    "        # backpropagate loss and optimize weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * tags.shape[0]\n",
    "        epoch_acc += acc.item() * tags.shape[0]\n",
    "        \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / len(dataloader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uM49eTuxTJFd",
    "outputId": "da82a551-70e9-43d3-a7df-74c48442e17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train [Loss:  0.512  Acc: 86.56]\n",
      "Epoch: 2, Train [Loss:  0.160  Acc: 95.52]\n",
      "Epoch: 3, Train [Loss:  0.097  Acc: 97.17]\n",
      "Epoch: 4, Train [Loss:  0.067  Acc: 97.91]\n",
      "Epoch: 5, Train [Loss:  0.050  Acc: 98.38]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, dataloader, optimizer, criterion, device)\n",
    "    print(f'Epoch: {epoch+1}, Train [Loss:  {train_loss:.3f}  Acc: {train_acc*100:.2f}]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTBKC17fTJFd"
   },
   "source": [
    "## Evaluate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Yx4cF6cUTJFd"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_batches, criterion, device):\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, tags in data_batches:\n",
    "            text = text.to(device)\n",
    "            tags = tags.to(device)\n",
    "\n",
    "            predictions = model(text).view(-1, TARGET_SIZE)\n",
    "            loss = criterion(predictions, tags.view(-1))\n",
    "\n",
    "            _, preds = torch.max(predictions, dim=1)\n",
    "            acc = accuracy_calculator(preds, tags.view(-1))\n",
    "\n",
    "            eval_loss += loss.item() * tags.shape[0]\n",
    "            eval_acc += acc.item() * tags.shape[0]\n",
    "    \n",
    "    return eval_loss / len(data_batches.dataset), eval_acc / len(data_batches.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usfU_c96TJFe",
    "outputId": "50d319e2-918a-478e-fa1c-0f6b6cd292d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 97.11%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_model(model, test_dataloader, criterion, device)\n",
    "print(f'Accuracy on test data: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4KMf6Vid3gI"
   },
   "source": [
    "# BERT for POS Tagging\n",
    "\n",
    "You can fine-tune a pretrained model from HuggingFace or train a model from zero. You **don't** need to implement the model from scratch.\n",
    "\n",
    "Refer to the fine-tuning BERT part of Lab 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OAwzQI45oeie"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqzeJpbToiE9",
    "outputId": "c06b6971-3cd9-4709-83fe-9e33e98d4238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-23 15:12:14--  https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1855828 (1.8M) [text/plain]\n",
      "Saving to: ‘train_pos.txt.4’\n",
      "\n",
      "\r",
      "train_pos.txt.4       0%[                    ]       0  --.-KB/s               \r",
      "train_pos.txt.4     100%[===================>]   1.77M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-04-23 15:12:14 (15.6 MB/s) - ‘train_pos.txt.4’ saved [1855828/1855828]\n",
      "\n",
      "--2023-04-23 15:12:14--  https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 418682 (409K) [text/plain]\n",
      "Saving to: ‘test_pos.txt.4’\n",
      "\n",
      "test_pos.txt.4      100%[===================>] 408.87K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2023-04-23 15:12:14 (11.7 MB/s) - ‘test_pos.txt.4’ saved [418682/418682]\n",
      "\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\n",
    "!wget https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_h66dvOSpLHi",
    "outputId": "2a28edf2-0fc8-44d7-d7bc-b532d738838b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.9/dist-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2.0.0+cu118)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.26.118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.22.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2.27.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch-pretrained-bert) (16.0.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch-pretrained-bert) (3.25.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.118 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-pretrained-bert) (1.29.118)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.118->boto3->pytorch-pretrained-bert) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LtRDzTn0oeie"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vocL1rNpoeif"
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    sentences_raw = content.split(\" \\n\")[:-1]\n",
    "    sentences, tags = [], []\n",
    "    for sent in sentences_raw:\n",
    "        words, pos_tags = [], []\n",
    "        for word_tag in sent.split(\"\\n\"):\n",
    "            if word_tag:\n",
    "                word, tag = word_tag.split(\" \")\n",
    "                words.append(word)\n",
    "                pos_tags.append(tag)\n",
    "        sentences.append(words)\n",
    "        tags.append(pos_tags)\n",
    "    return sentences, tags\n",
    "\n",
    "train_sentences, train_tags = read_data(\"train_pos.txt\")\n",
    "test_sentences, test_tags = read_data(\"test_pos.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RrSCEakyoeif"
   },
   "outputs": [],
   "source": [
    "tags = list(set(tag for sent in train_tags for tag in sent))\n",
    "# By convention, the 0'th slot is reserved for padding.\n",
    "tags = [\"<pad>\"] + tags\n",
    "\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "kKCUBVv_oeif"
   },
   "outputs": [],
   "source": [
    "class PosDataset(data.Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for words, tags_ in zip(sentences, tags):\n",
    "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "            tags_li.append([\"<pad>\"] + tags_ + [\"<pad>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "\n",
    "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZSQ_Np3Noeif"
   },
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aE_vy2Kmoeig"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        '''\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "        \n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vlefTlEwoeig"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(\"step: {}, loss: {}\".format(i, loss.item()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "d2bat6Iloeih"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net(vocab_size=len(tag2idx))\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "S1W6oTN2oeii"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = PosDataset(train_sentences, train_tags)\n",
    "eval_dataset = PosDataset(test_sentences, test_tags)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "test_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H10k0gg7oeii",
    "outputId": "d1a6dea0-c009-4afd-c905-b755e16ca088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.8752777576446533\n",
      "step: 10, loss: 1.6787902116775513\n",
      "step: 20, loss: 0.8217227458953857\n",
      "step: 30, loss: 0.3105090260505676\n",
      "step: 40, loss: 0.3609287738800049\n",
      "step: 50, loss: 0.20232824981212616\n",
      "step: 60, loss: 0.21270792186260223\n",
      "step: 70, loss: 0.1429995447397232\n",
      "step: 80, loss: 0.19636942446231842\n",
      "step: 90, loss: 0.1384022831916809\n",
      "step: 100, loss: 0.11066035181283951\n",
      "step: 110, loss: 0.1303025782108307\n",
      "step: 120, loss: 0.05210668966174126\n",
      "step: 130, loss: 0.11403147876262665\n",
      "step: 140, loss: 0.057761166244745255\n",
      "step: 150, loss: 0.1569521278142929\n",
      "step: 160, loss: 0.1303919404745102\n",
      "step: 170, loss: 0.09457496553659439\n",
      "step: 180, loss: 0.08607296645641327\n",
      "step: 190, loss: 0.09879419207572937\n",
      "step: 200, loss: 0.11601632088422775\n",
      "step: 210, loss: 0.07898501306772232\n",
      "step: 220, loss: 0.16456657648086548\n",
      "step: 230, loss: 0.08946497738361359\n",
      "step: 240, loss: 0.07853136956691742\n",
      "step: 250, loss: 0.10468841344118118\n",
      "step: 260, loss: 0.0906369537115097\n",
      "step: 270, loss: 0.073245570063591\n",
      "step: 280, loss: 0.10775354504585266\n",
      "step: 290, loss: 0.1296796202659607\n",
      "step: 300, loss: 0.10644009709358215\n",
      "step: 310, loss: 0.09484030306339264\n",
      "step: 320, loss: 0.11867519468069077\n",
      "step: 330, loss: 0.047107208520174026\n",
      "step: 340, loss: 0.17902450263500214\n",
      "step: 350, loss: 0.04495931416749954\n",
      "step: 360, loss: 0.02896268479526043\n",
      "step: 370, loss: 0.07696111500263214\n",
      "step: 380, loss: 0.059911780059337616\n",
      "step: 390, loss: 0.1031675711274147\n",
      "step: 400, loss: 0.14142419397830963\n",
      "step: 410, loss: 0.07597202062606812\n",
      "step: 420, loss: 0.2535100281238556\n",
      "step: 430, loss: 0.11190237104892731\n",
      "step: 440, loss: 0.050236091017723083\n",
      "step: 450, loss: 0.040776968002319336\n",
      "step: 460, loss: 0.06120019033551216\n",
      "step: 470, loss: 0.08140429109334946\n",
      "step: 480, loss: 0.12175597995519638\n",
      "step: 490, loss: 0.049618545919656754\n",
      "step: 500, loss: 0.17550478875637054\n",
      "step: 510, loss: 0.056855618953704834\n",
      "step: 520, loss: 0.13674506545066833\n",
      "step: 530, loss: 0.17126275599002838\n",
      "step: 540, loss: 0.10253466665744781\n",
      "step: 550, loss: 0.11415886133909225\n",
      "step: 560, loss: 0.0798250287771225\n",
      "step: 570, loss: 0.0798940509557724\n",
      "step: 580, loss: 0.05645168945193291\n",
      "step: 590, loss: 0.05437322333455086\n",
      "step: 600, loss: 0.1681073158979416\n",
      "step: 610, loss: 0.04349273070693016\n",
      "step: 620, loss: 0.027594244107604027\n",
      "step: 630, loss: 0.07926835119724274\n",
      "step: 640, loss: 0.08317339420318604\n",
      "step: 650, loss: 0.030248431488871574\n",
      "step: 660, loss: 0.13084381818771362\n",
      "step: 670, loss: 0.016060080379247665\n",
      "step: 680, loss: 0.10507567971944809\n",
      "step: 690, loss: 0.03776824101805687\n",
      "step: 700, loss: 0.06327971816062927\n",
      "step: 710, loss: 0.06746647506952286\n",
      "step: 720, loss: 0.08536328375339508\n",
      "step: 730, loss: 0.05243559554219246\n",
      "step: 740, loss: 0.07518566399812698\n",
      "step: 750, loss: 0.10592346638441086\n",
      "step: 760, loss: 0.07450157403945923\n",
      "step: 770, loss: 0.11503594368696213\n",
      "step: 780, loss: 0.03809739276766777\n",
      "step: 790, loss: 0.0678819864988327\n",
      "step: 800, loss: 0.03650747984647751\n",
      "step: 810, loss: 0.073882557451725\n",
      "step: 820, loss: 0.06284435093402863\n",
      "step: 830, loss: 0.044311411678791046\n",
      "step: 840, loss: 0.20424829423427582\n",
      "step: 850, loss: 0.08705690503120422\n",
      "step: 860, loss: 0.11643897742033005\n",
      "step: 870, loss: 0.13781756162643433\n",
      "step: 880, loss: 0.051650457084178925\n",
      "step: 890, loss: 0.07913444936275482\n",
      "step: 900, loss: 0.08697172999382019\n",
      "step: 910, loss: 0.02122887223958969\n",
      "step: 920, loss: 0.046518344432115555\n",
      "step: 930, loss: 0.12879584729671478\n",
      "step: 940, loss: 0.0829196497797966\n",
      "step: 950, loss: 0.11804290860891342\n",
      "step: 960, loss: 0.05141662061214447\n",
      "step: 970, loss: 0.11192850023508072\n",
      "step: 980, loss: 0.06623375415802002\n",
      "step: 990, loss: 0.02745272032916546\n",
      "step: 1000, loss: 0.02907373011112213\n",
      "step: 1010, loss: 0.15000060200691223\n",
      "step: 1020, loss: 0.173689603805542\n",
      "step: 1030, loss: 0.09008413553237915\n",
      "step: 1040, loss: 0.08062003552913666\n",
      "step: 1050, loss: 0.15743137896060944\n",
      "step: 1060, loss: 0.06660949438810349\n",
      "step: 1070, loss: 0.0993850827217102\n",
      "step: 1080, loss: 0.14144866168498993\n",
      "step: 1090, loss: 0.09978413581848145\n",
      "step: 1100, loss: 0.08244267106056213\n",
      "step: 1110, loss: 0.03625358268618584\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aveFSbozoeii"
   },
   "source": [
    "## Evaluate BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "UH8QayCrqhT_"
   },
   "outputs": [],
   "source": [
    "def eval(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(\"result\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
    "            fout.write(\"\\n\")\n",
    "            \n",
    "    ## calc metric\n",
    "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
    "\n",
    "    print(\"Accuracy=%.2f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7fkJjCLoeij",
    "outputId": "7d04f4aa-3ae6-42da-8d72-fa9456f4935b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.98\n"
     ]
    }
   ],
   "source": [
    "eval(model, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95HCITItd9W5"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvfBnV93eAxd"
   },
   "source": [
    "Write your opinions and conclusions about the application of HMM, LSTM and BERT to PoS Tagging\n",
    "- discuss the results\n",
    "- pros and cons of each model\n",
    "- 4-6 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMZ1Rsvvh7dy"
   },
   "source": [
    "Answer: The application of HMM, LSTM, and BERT to PoS Tagging yields varying results, with BERT achieving the highest accuracy at 98%, followed by LSTM at 97%, and HMM at 86%. The HMM's relatively lower accuracy can be attributed to its simplicity and inability to capture long-term dependencies, but it benefits from lower computational requirements and faster training times. In contrast, LSTM successfully addresses the long-term dependency issue and offers improved accuracy, but at the cost of increased computational complexity. BERT, as a pre-trained model, provides the best accuracy and generalization, although it comes with the highest computational demands and may require fine-tuning for specific tasks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
