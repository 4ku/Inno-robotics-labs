{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKboZnAdgrRM"
   },
   "source": [
    "# Tweets Tokenization\n",
    "\n",
    "The goal of the assignment is to write a tweet tokenizer. The input of the code will be a set of tweet text and the output will be the tokens in each tweet. The assignment is made up of four tasks.\n",
    "\n",
    "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline. For manual tokenization only one file should be used.\n",
    "\n",
    "Grading:\n",
    "- 30 points - Tokenize tweets by hand\n",
    "- 30 points - Implement 4 tokenizers\n",
    "- 20 points - Stemming and Lemmatization\n",
    "- 20 points - Explain sentencepiece (for masters only)\n",
    "\n",
    "\n",
    "Remarks: \n",
    "- Use Python 3 or greater\n",
    "- Max is 80 points for bachelors, 100 points for masters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLDjjAvemUP_"
   },
   "source": [
    "## Tokenize tweets by hand\n",
    "\n",
    "As a first task you need to tokenize 15 tweets by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
    "\n",
    "- Each smiley is a separate token\n",
    "- Each hashtag is an individual token. Each user reference is an individual token\n",
    "- If a word has spaces between them then it is converted to a single token\n",
    "- If a sentence ends with a word that legitimately has a full stop (abbreviations, for example), add a final full stop\n",
    "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
    "- A URL is a single token\n",
    "\n",
    "Example of output\n",
    "\n",
    "    Input tweet\n",
    "    @xfranman Old age has made N A T O!\n",
    "\n",
    "    Tokenized tweet (separated by comma)\n",
    "    @xfranman , Old , age , has , made , NATO , !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Input tweet\n",
    "    @anitapuspasari waduh..\n",
    "    1. Tokenized tweet\n",
    "    @anitapuspasari, waduh..\n",
    "\n",
    "    2. Input tweet\n",
    "    \" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\n",
    "    2. Tokenized tweet\n",
    "    \" , Could , journos , please , stop , putting , the , word , \", \" , gate , \", \" , after , everything , they , write , ... , gate , . , \"\n",
    "    \n",
    "    3. Input tweet\n",
    "    20% More Ridiculous Sale @20x200 ends tonight! - get 20% off by entering 'RIDONK' at checkout. More info: http://bit.ly/ridonktues\n",
    "\n",
    "    3. Tokenized tweet\n",
    "    20% , More , Ridiculous , Sale , @20x200 , ends , tonight , ! , - , get , 20% , off , by , entering , ', RIDONK, ' , at , checkout , . , More , info , : , http://bit.ly/ridonktues\n",
    "\n",
    "    4. Input tweet\n",
    "    @Studio85 I have a pair of those shoes. They are comfy. Like being barefoot. Okay for running, but not on concrete, as I've discovered.\n",
    "\n",
    "    4. Tokenized tweet\n",
    "    @Studio85, I, have, a, pair, of, those, shoes, ., They, are, comfy, ., Like, being, barefoot, ., Okay, for, running, ,, but, not, on, concrete, ,, as, I've, discovered, .\n",
    "    \n",
    "    5. Input tweet\n",
    "    RT @twilightus Team Carlisle is a Trending Topic- help him out RT Follow @peterfacinelli see a grown man n a bikini dance Hollywood Blvd\n",
    "\n",
    "    5. Tokenized tweet\n",
    "    RT, @twilightus, Team, Carlisle, is, a, Trending, Topic- , help, him, out, RT, Follow, @peterfacinelli, see, a, grown, man, n, a, bikini, dance, Hollywood, Blvd\n",
    "\n",
    "    6. Input tweet\n",
    "    @karenrubin you might have to reinstall - that happened to me a few months ago, now I use Nambu on my Mac\n",
    "\n",
    "    6. Tokenized tweet\n",
    "    @karenrubin, you, might, have, to, reinstall, -, that, happened, to, me, a, few, months, ago, ,, now, I, use, Nambu, on, my, Mac\n",
    "    \n",
    "    7. Input tweet\n",
    "    Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\n",
    "\n",
    "    7. Tokenized tweet\n",
    "    Just, Posted , :, Redneck, Dragon, -, Part, XXVIII, (, http://cli.gs/gWy0yT, )\n",
    "\n",
    "    8. Input tweet\n",
    "    \" \"\"Paul McCartney ... went through all his education there and nobody thought he had any musical talent,\"\" http://tinyurl.com/nkdbdq\"\n",
    "\n",
    "    8. Tokenized tweet\n",
    "    \" , \"\" , Paul, McCartney, ..., went, through, all, his, education, there, and, nobody, thought, he, had, any, musical, talent, , \"\" , http://tinyurl.com/nkdbdq, \"\"\n",
    "    \n",
    "    9. Input tweet\n",
    "    @ambienteer Yeah, pretty much how i feel about it.\n",
    "\n",
    "    9. Tokenized tweet\n",
    "    @ambienteer, Yeah, ,, pretty, much, how, i, feel, about, it, .\n",
    "\n",
    "    10. Input tweet\n",
    "    @florianseroussi Nothing really noticeable? Are you kidding?\n",
    "\n",
    "    10. Tokenized tweet\n",
    "    @florianseroussi, Nothing, really, noticeable, ?, Are, you, kidding, ?\n",
    "    \n",
    "    11. Input tweet\n",
    "    @toiletooth Hours?\n",
    "\n",
    "    11. Tokenized tweet\n",
    "    @toiletooth, Hours, ?\n",
    "\n",
    "    12. Input tweet\n",
    "    \" Obama,Hamas,and the Mullahs being \"\"helpfu l\"\"http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter\"\n",
    "\n",
    "    12. Tokenized tweet\n",
    "    \" , Obama, ,, Hamas, ,, and, the, Mullahs, being, \"\" , helpful , \"\" , http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter , \"\n",
    "    \n",
    "    13. Input tweet\n",
    "    RT @BBHLabs 81% of twitter users are UNDER 30 + more v. interesting statistics here: http://www.sysomos.com/insidetwitter/\n",
    "\n",
    "    13. Tokenized tweet\n",
    "    RT, @BBHLabs, 81%, of, twitter, users, are, UNDER, 30, +, more, v., interesting, statistics, here, :, http://www.sysomos.com/insidetwitter/\n",
    "\n",
    "    14. Input tweet\n",
    "    @Birdingperu Great looking hummer! RTThe world's most spectacular hummingbird Marvelous Spatuletail on a feeder. http://bit.ly/aGHYZ\n",
    "\n",
    "    14. Tokenized tweet\n",
    "    @Birdingperu, Great, looking, hummer, !, RTThe, world's, most, spectacular, hummingbird, Marvelous, Spatuletail, on, a, feeder, ., http://bit.ly/aGHYZ\n",
    "\n",
    "    15. Input tweet\n",
    "    attn. chas. whitman: RT @villagevoice Jonas Brothers at Rockefeller Center for the Today Show tomorrow morn—EEEEEEEEEE!\n",
    "\n",
    "    15. Tokenized tweet\n",
    "    attn., chas., whitman, :, RT, @villagevoice, Jonas, Brothers, at, Rockefeller, Center, for, the, Today, Show, tomorrow, morn, — , EEEEEEEEE , !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2J2AD2nmUhi"
   },
   "source": [
    "## Implement 4 tokenizers\n",
    "\n",
    "Your task is to implement the 4 different tokenizers that take a list of tweets on a topic and output tokenization for each:\n",
    "\n",
    "- White Space Tokenization\n",
    "- Sentencepiece\n",
    "- Tokenizing text using regular expressions\n",
    "- NLTK TweetTokenizer\n",
    "\n",
    "For tokenizing text using regular expressions use the rules in task 1. Combine task 1 rules into regular expression and create a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4uZId1tjrfyz"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def white_space_tokenizer(text: str) -> List[str]:\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all tweets and write data to one file\n",
    "data = []\n",
    "for i in range(1, 6):\n",
    "    with open(f\"Assignment1_data/file{i}\", \"r\") as f:\n",
    "        tweets = f.readlines()\n",
    "        data.extend(tweets)\n",
    "        with open(\"Assignment1_data/all_tweets\", \"a\") as output:\n",
    "            output.writelines(tweets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=Assignment1_data/all_tweets --model_prefix=tweet --vocab_size=500\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Assignment1_data/all_tweets\n",
      "  input_format: \n",
      "  model_prefix: tweet\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: Assignment1_data/all_tweets\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 1100 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=108120\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9538% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=157\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999538\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 1100 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 3441 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 1100\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 2003\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 2003 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2649 obj=18.2038 num_tokens=6619 num_tokens/piece=2.49868\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2283 obj=16.7263 num_tokens=6637 num_tokens/piece=2.90714\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1712 obj=16.8418 num_tokens=6827 num_tokens/piece=3.98773\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1710 obj=16.6977 num_tokens=6838 num_tokens/piece=3.99883\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1282 obj=17.1766 num_tokens=7290 num_tokens/piece=5.68643\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1282 obj=16.9635 num_tokens=7294 num_tokens/piece=5.68955\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=961 obj=17.7011 num_tokens=7975 num_tokens/piece=8.29865\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=961 obj=17.4307 num_tokens=7988 num_tokens/piece=8.31217\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=720 obj=18.1605 num_tokens=8705 num_tokens/piece=12.0903\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=720 obj=17.9037 num_tokens=8705 num_tokens/piece=12.0903\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=550 obj=18.6479 num_tokens=9420 num_tokens/piece=17.1273\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=550 obj=18.4354 num_tokens=9421 num_tokens/piece=17.1291\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: tweet.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: tweet.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train the SentencePiece model on the input text\n",
    "spm.SentencePieceTrainer.Train('--input=Assignment1_data/all_tweets --model_prefix=tweet --vocab_size=500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4ou1WE8Krf_W"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def sentencepiece_wrapper(text: str) -> List[str]:\n",
    "    # Load the pre-trained SentencePiece model for Twitter data\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"tweet.model\")\n",
    "\n",
    "    # Tokenize the text into pieces\n",
    "    tokens = sp.EncodeAsPieces(text)\n",
    "\n",
    "    # Return the list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pC32dK_urf5P"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def re_tokenizer(text: str) -> List[str]:\n",
    "    smiley = r':\\)|;\\)|:-\\)'\n",
    "    hashtag = r'#\\w+'\n",
    "    user_reference = r'@\\w+'\n",
    "    full_stop = r'\\w+.'\n",
    "    url = r'https?://[^\\s]+'\n",
    "    punc = r'\\W'\n",
    "    pattern = f\"{smiley}|{hashtag}|{user_reference}|{full_stop}|{url}|{punc}\"\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q8UVniWVrgMT"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def nltk_tweet_tokenizer(text: str) -> List[str]:\n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIhPteb4s_Yn"
   },
   "source": [
    "Run your implementations on the data. Compare the results, decide which one is better. List the advantages of the best tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:\n",
      "White space tokenizer:\n",
      "['@anitapuspasari', 'waduh..']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁@', 'an', 'ita', 'p', 'u', 's', 'p', 'a', 's', 'ar', 'i', '▁w', 'ad', 'u', 'h', '..']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['@anitapuspasari', ' ', 'waduh.', '.', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['@anitapuspasari', 'waduh', '..']\n",
      "------------------------------\n",
      "\n",
      "Tweet 2:\n",
      "White space tokenizer:\n",
      "['\"', 'Could', 'journos', 'please', 'stop', 'putting', 'the', 'word', '\"\"gate\"\"', 'after', 'everything', 'they', 'write...', 'gate.\"']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁\"', '▁C', 'ould', '▁', 'j', 'our', 'no', 's', '▁pleas', 'e', '▁sto', 'p', '▁put', 't', 'ing', '▁the', '▁w', 'o', 'rd', '▁\"\"', 'ga', 'te', '\"\"', '▁a', 'f', 'ter', '▁', 'e', 'very', 'th', 'ing', '▁they', '▁w', 'r', 'ite', '...', '▁g', 'at', 'e', '.', '\"']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['\"', ' ', 'Could ', 'journos ', 'please ', 'stop ', 'putting ', 'the ', 'word ', '\"', '\"', 'gate\"', '\"', ' ', 'after ', 'everything ', 'they ', 'write.', '.', '.', ' ', 'gate.', '\"', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['\"', 'Could', 'journos', 'please', 'stop', 'putting', 'the', 'word', '\"', '\"', 'gate', '\"', '\"', 'after', 'everything', 'they', 'write', '...', 'gate', '.', '\"']\n",
      "------------------------------\n",
      "\n",
      "Tweet 3:\n",
      "White space tokenizer:\n",
      "['20%', 'More', 'Ridiculous', 'Sale', '@20x200', 'ends', 'tonight!', '-', 'get', '20%', 'off', 'by', 'entering', \"'RIDONK'\", 'at', 'checkout.', 'More', 'info:', 'http://bit.ly/ridonktues']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁', '20', '%', '▁Mor', 'e', '▁', 'R', 'id', 'i', 'c', 'u', 'l', 'ous', '▁S', 'al', 'e', '▁@', '20', 'x', '20', '0', '▁', 'end', 's', '▁tonight', '!', '▁-', '▁get', '▁', '20', '%', '▁of', 'f', '▁b', 'y', '▁', 'ent', 'er', 'ing', '▁', \"'\", 'R', 'I', 'D', 'ON', 'K', \"'\", '▁at', '▁check', 'o', 'ut', '.', '▁Mor', 'e', '▁in', 'f', 'o', ':', '▁', 'ht', 'tp', '://', 'bit', '.', 'ly', '/', 'r', 'id', 'on', 'k', 't', 'u', 'es']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['20%', ' ', 'More ', 'Ridiculous ', 'Sale ', '@20x200', ' ', 'ends ', 'tonight!', ' ', '-', ' ', 'get ', '20%', ' ', 'off ', 'by ', 'entering ', \"'\", \"RIDONK'\", ' ', 'at ', 'checkout.', ' ', 'More ', 'info:', ' ', 'http:', '/', '/', 'bit.', 'ly/', 'ridonktues', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['20', '%', 'More', 'Ridiculous', 'Sale', '@20x200', 'ends', 'tonight', '!', '-', 'get', '20', '%', 'off', 'by', 'entering', \"'\", 'RIDONK', \"'\", 'at', 'checkout', '.', 'More', 'info', ':', 'http://bit.ly/ridonktues']\n",
      "------------------------------\n",
      "\n",
      "Tweet 4:\n",
      "White space tokenizer:\n",
      "['@Studio85', 'I', 'have', 'a', 'pair', 'of', 'those', 'shoes.', 'They', 'are', 'comfy.', 'Like', 'being', 'barefoot.', 'Okay', 'for', 'running,', 'but', 'not', 'on', 'concrete,', 'as', \"I've\", 'discovered.']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁@', 'S', 't', 'ud', 'i', 'o', '8', '5', '▁I', '▁have', '▁a', '▁pa', 'ir', '▁of', '▁', 'th', 'o', 'se', '▁sh', 'o', 'es', '.', '▁The', 'y', '▁are', '▁', 'com', 'f', 'y', '.', '▁', 'L', 'ik', 'e', '▁be', 'ing', '▁b', 'a', 're', 'f', 'oo', 't', '.', '▁O', 'k', 'a', 'y', '▁for', '▁', 'r', 'u', 'n', 'n', 'ing', ',', '▁', 'but', '▁not', '▁on', '▁con', 'c', 're', 'te', ',', '▁a', 's', '▁I', \"'\", 've', '▁d', 'is', 'co', 'v', 'er', 'ed', '.']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['@Studio85', ' ', 'I ', 'have ', 'a ', 'pair ', 'of ', 'those ', 'shoes.', ' ', 'They ', 'are ', 'comfy.', ' ', 'Like ', 'being ', 'barefoot.', ' ', 'Okay ', 'for ', 'running,', ' ', 'but ', 'not ', 'on ', 'concrete,', ' ', 'as ', \"I'\", 've ', 'discovered.', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['@Studio85', 'I', 'have', 'a', 'pair', 'of', 'those', 'shoes', '.', 'They', 'are', 'comfy', '.', 'Like', 'being', 'barefoot', '.', 'Okay', 'for', 'running', ',', 'but', 'not', 'on', 'concrete', ',', 'as', \"I've\", 'discovered', '.']\n",
      "------------------------------\n",
      "\n",
      "Tweet 5:\n",
      "White space tokenizer:\n",
      "['RT', '@twilightus', 'Team', 'Carlisle', 'is', 'a', 'Trending', 'Topic-', 'help', 'him', 'out', 'RT', 'Follow', '@peterfacinelli', 'see', 'a', 'grown', 'man', 'n', 'a', 'bikini', 'dance', 'Hollywood', 'Blvd']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁RT', '▁@', 't', 'w', 'il', 'ight', 'u', 's', '▁T', 'e', 'am', '▁C', 'ar', 'l', 'is', 'le', '▁is', '▁a', '▁T', 'r', 'end', 'ing', '▁T', 'o', 'pic', '-', '▁', 'help', '▁h', 'im', '▁out', '▁RT', '▁F', 'ol', 'lo', 'w', '▁@', 'p', 'e', 'ter', 'f', 'a', 'c', 'in', 'ell', 'i', '▁see', '▁a', '▁g', 'r', 'own', '▁man', '▁', 'n', '▁a', '▁b', 'ik', 'in', 'i', '▁d', 'ance', '▁H', 'ol', 'ly', 'w', 'ood', '▁', 'B', 'l', 'v', 'd']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['RT ', '@twilightus', ' ', 'Team ', 'Carlisle ', 'is ', 'a ', 'Trending ', 'Topic-', ' ', 'help ', 'him ', 'out ', 'RT ', 'Follow ', '@peterfacinelli', ' ', 'see ', 'a ', 'grown ', 'man ', 'n ', 'a ', 'bikini ', 'dance ', 'Hollywood ', 'Blvd', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['RT', '@twilightus', 'Team', 'Carlisle', 'is', 'a', 'Trending', 'Topic', '-', 'help', 'him', 'out', 'RT', 'Follow', '@peterfacinelli', 'see', 'a', 'grown', 'man', 'n', 'a', 'bikini', 'dance', 'Hollywood', 'Blvd']\n",
      "------------------------------\n",
      "\n",
      "Tweet 6:\n",
      "White space tokenizer:\n",
      "['@karenrubin', 'you', 'might', 'have', 'to', 'reinstall', '-', 'that', 'happened', 'to', 'me', 'a', 'few', 'months', 'ago,', 'now', 'I', 'use', 'Nambu', 'on', 'my', 'Mac']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁@', 'k', 'ar', 'en', 'r', 'ub', 'in', '▁you', '▁', 'm', 'ight', '▁have', '▁to', '▁re', 'in', 'st', 'all', '▁-', '▁that', '▁', 'ha', 'p', 'p', 'en', 'ed', '▁to', '▁me', '▁a', '▁fe', 'w', '▁mo', 'n', 'th', 's', '▁a', 'g', 'o', ',', '▁no', 'w', '▁I', '▁use', '▁N', 'am', 'b', 'u', '▁on', '▁my', '▁M', 'a', 'c']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['@karenrubin', ' ', 'you ', 'might ', 'have ', 'to ', 'reinstall ', '-', ' ', 'that ', 'happened ', 'to ', 'me ', 'a ', 'few ', 'months ', 'ago,', ' ', 'now ', 'I ', 'use ', 'Nambu ', 'on ', 'my ', 'Mac', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['@karenrubin', 'you', 'might', 'have', 'to', 'reinstall', '-', 'that', 'happened', 'to', 'me', 'a', 'few', 'months', 'ago', ',', 'now', 'I', 'use', 'Nambu', 'on', 'my', 'Mac']\n",
      "------------------------------\n",
      "\n",
      "Tweet 7:\n",
      "White space tokenizer:\n",
      "['Just', 'Posted:', 'Redneck', 'Dragon', '-', 'Part', 'XXVIII', '(http://cli.gs/gWy0yT)']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁', 'Just', '▁', 'P', 'o', 'st', 'ed', ':', '▁Re', 'd', 'ne', 'ck', '▁D', 'r', 'a', 'g', 'on', '▁-', '▁', 'P', 'art', '▁', 'X', 'X', 'V', 'I', 'I', 'I', '▁(', 'ht', 'tp', '://', 'c', 'l', 'i', '.', 'g', 's', '/', 'g', 'W', 'y', '0', 'y', 'T', ')']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['Just ', 'Posted:', ' ', 'Redneck ', 'Dragon ', '-', ' ', 'Part ', 'XXVIII ', '(', 'http:', '/', '/', 'cli.', 'gs/', 'gWy0yT)', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['Just', 'Posted', ':', 'Redneck', 'Dragon', '-', 'Part', 'XXVIII', '(', 'http://cli.gs/gWy0yT', ')']\n",
      "------------------------------\n",
      "\n",
      "Tweet 8:\n",
      "White space tokenizer:\n",
      "['\"', '\"\"Paul', 'McCartney', '...', 'went', 'through', 'all', 'his', 'education', 'there', 'and', 'nobody', 'thought', 'he', 'had', 'any', 'musical', 'talent,\"\"', 'http://tinyurl.com/nkdbdq\"']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁\"', '▁\"\"', 'P', 'a', 'u', 'l', '▁M', 'c', 'C', 'art', 'ne', 'y', '▁', '...', '▁we', 'nt', '▁', 't', 'hrough', '▁all', '▁', 'his', '▁', 'ed', 'u', 'c', 'a', 'tion', '▁there', '▁', 'and', '▁no', 'body', '▁', 'th', 'o', 'u', 'g', 'ht', '▁he', '▁h', 'ad', '▁any', '▁', 'm', 'u', 's', 'i', 'c', 'al', '▁', 't', 'al', 'ent', ',', '\"\"', '▁', 'ht', 'tp', '://', 'tiny', 'ur', 'l', '.', 'com', '/', 'n', 'k', 'd', 'b', 'd', 'q', '\"']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['\"', ' ', '\"', '\"', 'Paul ', 'McCartney ', '.', '.', '.', ' ', 'went ', 'through ', 'all ', 'his ', 'education ', 'there ', 'and ', 'nobody ', 'thought ', 'he ', 'had ', 'any ', 'musical ', 'talent,', '\"', '\"', ' ', 'http:', '/', '/', 'tinyurl.', 'com/', 'nkdbdq\"', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['\"', '\"', '\"', 'Paul', 'McCartney', '...', 'went', 'through', 'all', 'his', 'education', 'there', 'and', 'nobody', 'thought', 'he', 'had', 'any', 'musical', 'talent', ',', '\"', '\"', 'http://tinyurl.com/nkdbdq', '\"']\n",
      "------------------------------\n",
      "\n",
      "Tweet 9:\n",
      "White space tokenizer:\n",
      "['@ambienteer', 'Yeah,', 'pretty', 'much', 'how', 'i', 'feel', 'about', 'it.']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁@', 'am', 'b', 'i', 'ent', 'e', 'er', '▁', 'Y', 'e', 'a', 'h', ',', '▁', 'pre', 't', 't', 'y', '▁', 'm', 'uch', '▁ho', 'w', '▁', 'i', '▁feel', '▁abo', 'ut', '▁it', '.']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['@ambienteer', ' ', 'Yeah,', ' ', 'pretty ', 'much ', 'how ', 'i ', 'feel ', 'about ', 'it.', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['@ambienteer', 'Yeah', ',', 'pretty', 'much', 'how', 'i', 'feel', 'about', 'it', '.']\n",
      "------------------------------\n",
      "\n",
      "Tweet 10:\n",
      "White space tokenizer:\n",
      "['@florianseroussi', 'Nothing', 'really', 'noticeable?', 'Are', 'you', 'kidding?']\n",
      "\n",
      "Sentencepiece:\n",
      "['▁@', 'f', 'lo', 'r', 'i', 'an', 's', 'er', 'ous', 's', 'i', '▁Nothing', '▁re', 'all', 'y', '▁not', 'ice', 'a', 'ble', '?', '▁A', 're', '▁you', '▁k', 'id', 'd', 'ing', '?']\n",
      "\n",
      "Regular expression tokenizer:\n",
      "['@florianseroussi', ' ', 'Nothing ', 'really ', 'noticeable?', ' ', 'Are ', 'you ', 'kidding?', '\\n']\n",
      "\n",
      "NLTK TweetTokenizer:\n",
      "['@florianseroussi', 'Nothing', 'really', 'noticeable', '?', 'Are', 'you', 'kidding', '?']\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(data[:10], 1):\n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(\"White space tokenizer:\")\n",
    "    print(white_space_tokenizer(tweet))\n",
    "    print()\n",
    "    print(\"Sentencepiece:\")\n",
    "    print(sentencepiece_wrapper(tweet))\n",
    "    print()\n",
    "    print(\"Regular expression tokenizer:\")\n",
    "    print(re_tokenizer(tweet))\n",
    "    print()\n",
    "    print(\"NLTK TweetTokenizer:\")\n",
    "    print(nltk_tweet_tokenizer(tweet))\n",
    "    print(\"-\"*30)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muAZeHkMtaCd"
   },
   "source": [
    "NLTK TweetTokenizer seems the best tokenizer for tweets. Here are some advantages:\n",
    "\n",
    "1. Handles emojis and emoticons: The TweetTokenizer is specifically designed to handle emojis and emoticons, which are commonly used in tweets and other social media texts.\n",
    "\n",
    "2. Handles hashtags and mentions: It also handles hashtags and mentions, which are unique features of social media texts.\n",
    "\n",
    "3. Preserves contractions: The TweetTokenizer preserves contractions, such as \"don't\" and \"can't\", which is important for preserving the meaning of the text.\n",
    "\n",
    "4. Supports different languages: The TweetTokenizer supports multiple languages, making it a suitable option for multilingual text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlzpzjgEpY-R"
   },
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Your task is to write two functions: stem and lemmatize. Input is a text, so you need to tokenize it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ghAR1rSjsnz1"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stem(text: str) -> List[str]:\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kZkR3TPYuk5T"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize(text: str) -> List[str]:\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:\n",
      "@anitapuspasari waduh..\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['@', 'anitapuspasari', 'waduh', '..']\n",
      "\n",
      "Lemmatized Tokens: ['@anitapuspasari', 'waduh', '..', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 2:\n",
      "\" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['``', 'could', 'journo', 'pleas', 'stop', 'put', 'the', 'word', '``', \"''\", 'gate', \"''\", \"''\", 'after', 'everyth', 'they', 'write', '...', 'gate', '.', \"''\"]\n",
      "\n",
      "Lemmatized Tokens: ['\"', 'could', 'journo', 'please', 'stop', 'put', 'the', 'word', '\"', '\"', 'gate', '\"', '\"', 'after', 'everything', 'they', 'write', '...', 'gate', '.', '\"', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 3:\n",
      "20% More Ridiculous Sale @20x200 ends tonight! - get 20% off by entering 'RIDONK' at checkout. More info: http://bit.ly/ridonktues\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['20', '%', 'more', 'ridicul', 'sale', '@', '20x200', 'end', 'tonight', '!', '-', 'get', '20', '%', 'off', 'by', 'enter', 'ridonk', \"'\", 'at', 'checkout', '.', 'more', 'info', ':', 'http', ':', '//bit.ly/ridonktu']\n",
      "\n",
      "Lemmatized Tokens: ['20', '%', 'More', 'ridiculous', 'Sale', '@20x200', 'end', 'tonight', '!', '-', 'get', '20', '%', 'off', 'by', 'enter', \"'\", 'RIDONK', \"'\", 'at', 'checkout', '.', 'More', 'info', ':', 'http://bit.ly/ridonktues', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 4:\n",
      "@Studio85 I have a pair of those shoes. They are comfy. Like being barefoot. Okay for running, but not on concrete, as I've discovered.\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['@', 'studio85', 'i', 'have', 'a', 'pair', 'of', 'those', 'shoe', '.', 'they', 'are', 'comfi', '.', 'like', 'be', 'barefoot', '.', 'okay', 'for', 'run', ',', 'but', 'not', 'on', 'concret', ',', 'as', 'i', 've', 'discov', '.']\n",
      "\n",
      "Lemmatized Tokens: ['@studio85', 'I', 'have', 'a', 'pair', 'of', 'those', 'shoe', '.', 'they', 'be', 'comfy', '.', 'like', 'be', 'barefoot', '.', 'okay', 'for', 'running', ',', 'but', 'not', 'on', 'concrete', ',', 'as', 'I', \"'ve\", 'discover', '.', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 5:\n",
      "RT @twilightus Team Carlisle is a Trending Topic- help him out RT Follow @peterfacinelli see a grown man n a bikini dance Hollywood Blvd\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['rt', '@', 'twilightus', 'team', 'carlisl', 'is', 'a', 'trend', 'topic-', 'help', 'him', 'out', 'rt', 'follow', '@', 'peterfacinelli', 'see', 'a', 'grown', 'man', 'n', 'a', 'bikini', 'danc', 'hollywood', 'blvd']\n",
      "\n",
      "Lemmatized Tokens: ['RT', '@twilightus', 'Team', 'Carlisle', 'be', 'a', 'Trending', 'Topic-', 'help', 'he', 'out', 'RT', 'Follow', '@peterfacinelli', 'see', 'a', 'grown', 'man', 'n', 'a', 'bikini', 'dance', 'Hollywood', 'Blvd', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 6:\n",
      "@karenrubin you might have to reinstall - that happened to me a few months ago, now I use Nambu on my Mac\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['@', 'karenrubin', 'you', 'might', 'have', 'to', 'reinstal', '-', 'that', 'happen', 'to', 'me', 'a', 'few', 'month', 'ago', ',', 'now', 'i', 'use', 'nambu', 'on', 'my', 'mac']\n",
      "\n",
      "Lemmatized Tokens: ['@karenrubin', 'you', 'might', 'have', 'to', 'reinstall', '-', 'that', 'happen', 'to', 'I', 'a', 'few', 'month', 'ago', ',', 'now', 'I', 'use', 'Nambu', 'on', 'my', 'Mac', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 7:\n",
      "Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['just', 'post', ':', 'redneck', 'dragon', '-', 'part', 'xxviii', '(', 'http', ':', '//cli.gs/gwy0yt', ')']\n",
      "\n",
      "Lemmatized Tokens: ['just', 'post', ':', 'Redneck', 'Dragon', '-', 'Part', 'XXVIII', '(', 'http://cli.gs/gwy0yt', ')', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 8:\n",
      "\" \"\"Paul McCartney ... went through all his education there and nobody thought he had any musical talent,\"\" http://tinyurl.com/nkdbdq\"\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['``', '``', \"''\", 'paul', 'mccartney', '...', 'went', 'through', 'all', 'his', 'educ', 'there', 'and', 'nobodi', 'thought', 'he', 'had', 'ani', 'music', 'talent', ',', \"''\", \"''\", 'http', ':', '//tinyurl.com/nkdbdq', \"''\"]\n",
      "\n",
      "Lemmatized Tokens: ['\"', '\"', '\"', 'Paul', 'McCartney', '...', 'go', 'through', 'all', 'his', 'education', 'there', 'and', 'nobody', 'think', 'he', 'have', 'any', 'musical', 'talent', ',', '\"', '\"', 'http://tinyurl.com/nkdbdq', '\"', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 9:\n",
      "@ambienteer Yeah, pretty much how i feel about it.\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['@', 'ambient', 'yeah', ',', 'pretti', 'much', 'how', 'i', 'feel', 'about', 'it', '.']\n",
      "\n",
      "Lemmatized Tokens: ['@ambienteer', 'yeah', ',', 'pretty', 'much', 'how', 'I', 'feel', 'about', 'it', '.', '\\n']\n",
      "------------------------------\n",
      "\n",
      "Tweet 10:\n",
      "@florianseroussi Nothing really noticeable? Are you kidding?\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['@', 'florianseroussi', 'noth', 'realli', 'notic', '?', 'are', 'you', 'kid', '?']\n",
      "\n",
      "Lemmatized Tokens: ['@florianseroussi', 'nothing', 'really', 'noticeable', '?', 'be', 'you', 'kid', '?', '\\n']\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(data[:10], 1):\n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(tweet)\n",
    "    print()\n",
    "    stemmed_tokens = stem(tweet)\n",
    "    print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "    print()\n",
    "    lemmatized_tokens = lemmatize(tweet)\n",
    "    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "    print(\"-\"*30)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "islrHZ6UmUoh"
   },
   "source": [
    "## Explain sentencepiece (for masters only)\n",
    "\n",
    "For this task you will have to use sentencepiece text tokenizer. Your task will be to read how it works and write a minimum 10 sentences explanation of the tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=Assignment1_data/all_tweets --model_prefix=tweet --vocab_size=500\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Assignment1_data/all_tweets\n",
      "  input_format: \n",
      "  model_prefix: tweet\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: Assignment1_data/all_tweets\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 1100 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=108120\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9538% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=157\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999538\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 1100 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 3441 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 1100\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 2003\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 2003 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2649 obj=18.2038 num_tokens=6619 num_tokens/piece=2.49868\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2283 obj=16.7263 num_tokens=6637 num_tokens/piece=2.90714\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1712 obj=16.8418 num_tokens=6827 num_tokens/piece=3.98773\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1710 obj=16.6977 num_tokens=6838 num_tokens/piece=3.99883\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1282 obj=17.1766 num_tokens=7290 num_tokens/piece=5.68643\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1282 obj=16.9635 num_tokens=7294 num_tokens/piece=5.68955\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=961 obj=17.7011 num_tokens=7975 num_tokens/piece=8.29865\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=961 obj=17.4307 num_tokens=7988 num_tokens/piece=8.31217\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=720 obj=18.1605 num_tokens=8705 num_tokens/piece=12.0903\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=720 obj=17.9037 num_tokens=8705 num_tokens/piece=12.0903\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=550 obj=18.6479 num_tokens=9420 num_tokens/piece=17.1273\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=550 obj=18.4354 num_tokens=9421 num_tokens/piece=17.1291\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: tweet.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: tweet.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train the SentencePiece model on the input text\n",
    "spm.SentencePieceTrainer.Train('--input=Assignment1_data/all_tweets --model_prefix=tweet --vocab_size=500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def sentencepiece_wrapper(text: str) -> List[str]:\n",
    "    # Load the pre-trained SentencePiece model for Twitter data\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"tweet.model\")\n",
    "\n",
    "    # Tokenize the text into pieces\n",
    "    tokens = sp.EncodeAsPieces(text)\n",
    "\n",
    "    # Return the list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:\n",
      "@anitapuspasari waduh..\n",
      "\n",
      "\n",
      "['▁@', 'an', 'ita', 'p', 'u', 's', 'p', 'a', 's', 'ar', 'i', '▁w', 'ad', 'u', 'h', '..']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 2:\n",
      "\" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\n",
      "\n",
      "\n",
      "['▁\"', '▁C', 'ould', '▁', 'j', 'our', 'no', 's', '▁pleas', 'e', '▁sto', 'p', '▁put', 't', 'ing', '▁the', '▁w', 'o', 'rd', '▁\"\"', 'ga', 'te', '\"\"', '▁a', 'f', 'ter', '▁', 'e', 'very', 'th', 'ing', '▁they', '▁w', 'r', 'ite', '...', '▁g', 'at', 'e', '.', '\"']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 3:\n",
      "20% More Ridiculous Sale @20x200 ends tonight! - get 20% off by entering 'RIDONK' at checkout. More info: http://bit.ly/ridonktues\n",
      "\n",
      "\n",
      "['▁', '20', '%', '▁Mor', 'e', '▁', 'R', 'id', 'i', 'c', 'u', 'l', 'ous', '▁S', 'al', 'e', '▁@', '20', 'x', '20', '0', '▁', 'end', 's', '▁tonight', '!', '▁-', '▁get', '▁', '20', '%', '▁of', 'f', '▁b', 'y', '▁', 'ent', 'er', 'ing', '▁', \"'\", 'R', 'I', 'D', 'ON', 'K', \"'\", '▁at', '▁check', 'o', 'ut', '.', '▁Mor', 'e', '▁in', 'f', 'o', ':', '▁', 'ht', 'tp', '://', 'bit', '.', 'ly', '/', 'r', 'id', 'on', 'k', 't', 'u', 'es']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 4:\n",
      "@Studio85 I have a pair of those shoes. They are comfy. Like being barefoot. Okay for running, but not on concrete, as I've discovered.\n",
      "\n",
      "\n",
      "['▁@', 'S', 't', 'ud', 'i', 'o', '8', '5', '▁I', '▁have', '▁a', '▁pa', 'ir', '▁of', '▁', 'th', 'o', 'se', '▁sh', 'o', 'es', '.', '▁The', 'y', '▁are', '▁', 'com', 'f', 'y', '.', '▁', 'L', 'ik', 'e', '▁be', 'ing', '▁b', 'a', 're', 'f', 'oo', 't', '.', '▁O', 'k', 'a', 'y', '▁for', '▁', 'r', 'u', 'n', 'n', 'ing', ',', '▁', 'but', '▁not', '▁on', '▁con', 'c', 're', 'te', ',', '▁a', 's', '▁I', \"'\", 've', '▁d', 'is', 'co', 'v', 'er', 'ed', '.']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 5:\n",
      "RT @twilightus Team Carlisle is a Trending Topic- help him out RT Follow @peterfacinelli see a grown man n a bikini dance Hollywood Blvd\n",
      "\n",
      "\n",
      "['▁RT', '▁@', 't', 'w', 'il', 'ight', 'u', 's', '▁T', 'e', 'am', '▁C', 'ar', 'l', 'is', 'le', '▁is', '▁a', '▁T', 'r', 'end', 'ing', '▁T', 'o', 'pic', '-', '▁', 'help', '▁h', 'im', '▁out', '▁RT', '▁F', 'ol', 'lo', 'w', '▁@', 'p', 'e', 'ter', 'f', 'a', 'c', 'in', 'ell', 'i', '▁see', '▁a', '▁g', 'r', 'own', '▁man', '▁', 'n', '▁a', '▁b', 'ik', 'in', 'i', '▁d', 'ance', '▁H', 'ol', 'ly', 'w', 'ood', '▁', 'B', 'l', 'v', 'd']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 6:\n",
      "@karenrubin you might have to reinstall - that happened to me a few months ago, now I use Nambu on my Mac\n",
      "\n",
      "\n",
      "['▁@', 'k', 'ar', 'en', 'r', 'ub', 'in', '▁you', '▁', 'm', 'ight', '▁have', '▁to', '▁re', 'in', 'st', 'all', '▁-', '▁that', '▁', 'ha', 'p', 'p', 'en', 'ed', '▁to', '▁me', '▁a', '▁fe', 'w', '▁mo', 'n', 'th', 's', '▁a', 'g', 'o', ',', '▁no', 'w', '▁I', '▁use', '▁N', 'am', 'b', 'u', '▁on', '▁my', '▁M', 'a', 'c']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 7:\n",
      "Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\n",
      "\n",
      "\n",
      "['▁', 'Just', '▁', 'P', 'o', 'st', 'ed', ':', '▁Re', 'd', 'ne', 'ck', '▁D', 'r', 'a', 'g', 'on', '▁-', '▁', 'P', 'art', '▁', 'X', 'X', 'V', 'I', 'I', 'I', '▁(', 'ht', 'tp', '://', 'c', 'l', 'i', '.', 'g', 's', '/', 'g', 'W', 'y', '0', 'y', 'T', ')']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 8:\n",
      "\" \"\"Paul McCartney ... went through all his education there and nobody thought he had any musical talent,\"\" http://tinyurl.com/nkdbdq\"\n",
      "\n",
      "\n",
      "['▁\"', '▁\"\"', 'P', 'a', 'u', 'l', '▁M', 'c', 'C', 'art', 'ne', 'y', '▁', '...', '▁we', 'nt', '▁', 't', 'hrough', '▁all', '▁', 'his', '▁', 'ed', 'u', 'c', 'a', 'tion', '▁there', '▁', 'and', '▁no', 'body', '▁', 'th', 'o', 'u', 'g', 'ht', '▁he', '▁h', 'ad', '▁any', '▁', 'm', 'u', 's', 'i', 'c', 'al', '▁', 't', 'al', 'ent', ',', '\"\"', '▁', 'ht', 'tp', '://', 'tiny', 'ur', 'l', '.', 'com', '/', 'n', 'k', 'd', 'b', 'd', 'q', '\"']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 9:\n",
      "@ambienteer Yeah, pretty much how i feel about it.\n",
      "\n",
      "\n",
      "['▁@', 'am', 'b', 'i', 'ent', 'e', 'er', '▁', 'Y', 'e', 'a', 'h', ',', '▁', 'pre', 't', 't', 'y', '▁', 'm', 'uch', '▁ho', 'w', '▁', 'i', '▁feel', '▁abo', 'ut', '▁it', '.']\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet 10:\n",
      "@florianseroussi Nothing really noticeable? Are you kidding?\n",
      "\n",
      "\n",
      "['▁@', 'f', 'lo', 'r', 'i', 'an', 's', 'er', 'ous', 's', 'i', '▁Nothing', '▁re', 'all', 'y', '▁not', 'ice', 'a', 'ble', '?', '▁A', 're', '▁you', '▁k', 'id', 'd', 'ing', '?']\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(data[:10], 1):\n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(tweet)\n",
    "    print()\n",
    "    print(sentencepiece_wrapper(tweet))\n",
    "    print(\"-\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2RjMwlEshCu"
   },
   "source": [
    "SentencePiece is a text tokenizer library that uses an unsupervised learning method to segment a sentence into subwords. It is designed for the processing of text in NLP, specifically for languages with no clear boundary between words. The library's purpose is to provide a flexible, fast and accurate tokenization solution for text data, regardless of the language.\n",
    "\n",
    "1. SentencePiece is an unsupervised learning algorithm, meaning it does not rely on annotated data to train.\n",
    "2.It segments a sentence into subwords or pieces, based on their frequency and co-occurrence patterns in the data.\n",
    "3.The model created by SentencePiece can handle different languages and scripts, making it highly versatile.\n",
    "4.The tokenization process uses a probability-based approach, which ensures that the most frequently occurring pieces are assigned the shortest representation.\n",
    "5.SentencePiece allows for a user-defined vocabulary size, ensuring that the model only uses the most important pieces of information.\n",
    "6.It supports both character-based and subword-based tokenization, making it suitable for a wide range of NLP tasks.\n",
    "7.SentencePiece provides a compact representation of text data, reducing the size of the text data to be processed.\n",
    "8.The model created by SentencePiece can be easily saved and loaded, making it possible to use the same tokenization process across different models or languages.\n",
    "9.SentencePiece is compatible with many NLP libraries and frameworks, making it a useful addition to any NLP project.\n",
    "10.The use of SentencePiece can significantly improve the accuracy of NLP tasks, such as language modeling, machine translation, and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmNpWzfLmUyE"
   },
   "source": [
    "## Resources\n",
    "\n",
    "1. [Regular Expressions 1](https://realpython.com/regex-python/)\n",
    "2. [Regular Expressions 2](https://realpython.com/regex-python-part-2/)\n",
    "2. [Spacy Lemmatizer](https://spacy.io/api/lemmatizer)\n",
    "2. [NLTK Stem](https://www.nltk.org/howto/stem.html)\n",
    "3. [SentencePiece](https://github.com/google/sentencepiece)\n",
    "4. [sentencepiece tokenizer](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
