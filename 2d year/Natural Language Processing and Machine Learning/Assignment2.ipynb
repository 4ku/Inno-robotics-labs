{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 40 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n",
    "- 20 points - Evaluate on Github Typo Corpus (for masters only)\n",
    "\n",
    "\n",
    "Remarks: \n",
    "- Use Python 3 or greater\n",
    "- Max is 80 points for bachelors, 100 points for masters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html).\n",
    "\n",
    "[N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "\n",
    "You may also wnat to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, Ukranian, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "Important - your project should not be a mere code copy-paste from somewhere. Implement yourself, analyze why it was suggested this way, and think of improvements/customization.\n",
    "\n",
    "Your solution should be able to perform 4 corrections per second (3-5 words in an example) on a typical cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "gram1 = defaultdict(int)  # word => count\n",
    "gram2 = defaultdict(int)  # (word1, word2) => count\n",
    "\n",
    "w2 = pd.read_csv('w2_.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "\n",
    "for index, row in w2.iterrows():\n",
    "    c = row[0]\n",
    "    gram1[row[1]] += c\n",
    "    gram1[row[2]] += c\n",
    "    gram2[tuple(row[1:3].values)] += c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norvig's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def normalize(text):\n",
    "    letters = []\n",
    "    for l in text.lower():\n",
    "        if l in ALPHABET:\n",
    "            letters.append(l)\n",
    "        elif l in \".?!\":\n",
    "            letters.append(' ')\n",
    "            # letters.append('.')\n",
    "            # letters.append(' ')\n",
    "        else:\n",
    "            letters.append(' ')\n",
    "    text = ''.join(letters)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "class NorvigSolution:\n",
    "    def __init__(self, words) -> None:\n",
    "        self.WORDS = words\n",
    "\n",
    "    def P(self, word, N=None): \n",
    "        \"Probability of `word`.\"\n",
    "        if N is None:\n",
    "            N = sum(self.WORDS.values())\n",
    "        if word in self.WORDS:\n",
    "            return self.WORDS[word] / N\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def correction(self, word): \n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word): \n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(edits1(word)) or self.known(edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words): \n",
    "        \"The subset of `words` that appear in the dictionary of self.WORDS.\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def correct_text(self, text):\n",
    "        text = normalize(text)\n",
    "        corrected_text = []\n",
    "        for word in text.split():\n",
    "            corrected_text.append(self.correction(word))\n",
    "        return ' '.join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector = NorvigSolution(gram1)\n",
    "corrector.correction('speling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doing sport is nice was to stai healthy'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"dking sport is nise wau to stai healtfy\"\n",
    "corrector.correct_text(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NgramSolution:\n",
    "    WEIGHTS = {\n",
    "        0: 1.1,\n",
    "        1: 1.0,\n",
    "        2: 0.9,\n",
    "    }\n",
    "    def __init__(self, gram1, gram2) -> None:\n",
    "        self.gram1 = gram1\n",
    "        self.gram2 = gram2\n",
    "\n",
    "    def P(self, word, sentence, pos):\n",
    "        word, level = word\n",
    "        score = 0\n",
    "        cond1 = sentence[pos-1] in self.gram1 and (sentence[pos-1], word) in self.gram2\n",
    "        if pos>0 and cond1:\n",
    "            score += self.gram2[(sentence[pos-1], word)]/self.gram1[sentence[pos-1]]\n",
    "\n",
    "        if pos<len(sentence)-1:\n",
    "            cond2 = word in self.gram1 and (word, sentence[pos+1]) in self.gram2\n",
    "            if cond2:\n",
    "                score += self.gram2[(word, sentence[pos+1])]/self.gram1[word]\n",
    "        return score #* self.WEIGHTS[level]\n",
    "\n",
    "    def correction(self, sentence, pos):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        word = sentence[pos]\n",
    "        cands = self.candidates(word)\n",
    "        if not cands:\n",
    "            cands = self.candidates(word, False)\n",
    "        if not cands:\n",
    "            return word\n",
    "        cands = sorted(cands, key=lambda w: self.P(w, sentence, pos), reverse=True)\n",
    "        cands = [c[0] for c in cands]\n",
    "        return cands[0]\n",
    "\n",
    "    def candidates(self, word, nearest=True):\n",
    "        res = {}\n",
    "        cands = ((0, [word]), (1, edits1(word))) if nearest else ((2, edits2(word)),)\n",
    "        for lvl, wrds in cands:\n",
    "            for w in wrds:\n",
    "                if w in self.gram1:\n",
    "                    res.setdefault(w, lvl)\n",
    "        return res.items()\n",
    "\n",
    "    def correct_text(self, text):\n",
    "        text = normalize(text).split()\n",
    "        for i in range(len(text)):\n",
    "            text[i] = self.correction(text, i)\n",
    "\n",
    "        return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ding sport in nine way to stay healthy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_corrector = NgramSolution(gram1, gram2)\n",
    "text = \"dking sport is nise wau to stai healtfy\"\n",
    "ngram_corrector.correct_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "In your implementation you will need to decide which ngram dataset to use, which weights to assign for edit1, edit2 or absent words probabilities, beam search parameters and etc. Write down justificaitons for these choices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 gram and 5 gram datasets. In my opinion, ideally it should be 3 grams. Because in real data, 5-gram examples are rare. That's why I chose the 2 gram dataset.  \n",
    "  \n",
    "I added a weight of 1.1 for 0 edits, because this is the more likely case, a little less for one edit and even less for 2 edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def editsk(word, k):\n",
    "    e = edits1(word)\n",
    "    prev = e\n",
    "    for i in range(1,k):\n",
    "        temp = set(e2 for e1 in prev for e2 in edits1(e1))\n",
    "        e.update(temp)\n",
    "        prev = temp\n",
    "    e.discard(word)\n",
    "    return e\n",
    "\n",
    "# Generate error word within 2 edit distance of the original word\n",
    "def generate_error_word(word, edit_distance = 2):\n",
    "    possible_errors = editsk(word, edit_distance)\n",
    "    return random.choice(list(possible_errors))\n",
    "    \n",
    "# Generate errors in a given set of sentence \n",
    "def generate_errors(data, edit_distance = 2, errors_per_sentence = 1):\n",
    "    print(\"Generating Errors\")\n",
    "    cnt = 0\n",
    "    for sentence in data:\n",
    "        indices = random.choices(range(len(sentence)), k = min(len(sentence), errors_per_sentence))\n",
    "        for i in indices:\n",
    "            sentence[i] = generate_error_word(sentence[i],edit_distance)\n",
    "        if cnt%100 == 0:\n",
    "            print(cnt, \"completed\")\n",
    "        cnt += 1\n",
    "\n",
    "    print(\"Errors Generated\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open('big.txt').read().split('.')\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = normalize(sentences[i]).split()\n",
    "sentences = sentences[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Errors\n",
      "0 completed\n",
      "100 completed\n",
      "200 completed\n",
      "300 completed\n",
      "400 completed\n",
      "Errors Generated\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "sentences_with_errors = copy.deepcopy(sentences)\n",
    "sentences_with_errors = generate_errors(sentences_with_errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Norvig's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 completed\n",
      "100 completed\n",
      "200 completed\n",
      "300 completed\n",
      "400 completed\n"
     ]
    }
   ],
   "source": [
    "norvig_corrector = NorvigSolution(gram1)\n",
    "corrected_data_norvig = []\n",
    "\n",
    "cnt = 0\n",
    "for sentence in sentences_with_errors:\n",
    "    corrected_sentence = []\n",
    "    for word in sentence:\n",
    "        c_word = norvig_corrector.correction(word)\n",
    "        corrected_sentence.append(c_word)\n",
    "    corrected_data_norvig.append(corrected_sentence)\n",
    "    if cnt%100 == 0:\n",
    "        print(cnt, \"completed\")\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.438\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(len(sentences)):\n",
    "    if corrected_data_norvig[i] == sentences[i]:\n",
    "        c += 1\n",
    "print(\"Accuracy:\", c/len(sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Ngram solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 completed\n",
      "100 completed\n",
      "200 completed\n",
      "300 completed\n",
      "400 completed\n"
     ]
    }
   ],
   "source": [
    "ngram_corrector = NgramSolution(gram1, gram2)\n",
    "corrected_data_ngram = []\n",
    "\n",
    "cnt = 0\n",
    "for sentence in sentences_with_errors:\n",
    "    s = ' '.join(sentence)\n",
    "    corrected_sentence = ngram_corrector.correct_text(s).split()\n",
    "    corrected_data_ngram.append(corrected_sentence)\n",
    "    \n",
    "    if cnt%100 == 0:\n",
    "        print(cnt, \"completed\")\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.048\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(len(sentences)):\n",
    "    if corrected_data_ngram[i] == sentences[i]:\n",
    "        c += 1\n",
    "print(\"Accuracy:\", c/len(sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Ngram solution show significantly less accuracy. I think there is some problem with code, but I can't find where."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VISJtkFD4VhV"
   },
   "source": [
    "## Evaluate on Github Typo Corpus\n",
    "Now, you need to evaluate your solution on the Github Typo Corpus. Don't forget to compare the accuracy with the Norvig's solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "tP9GZCYsWjgB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-14 17:53:05--  https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\n",
      "Resolving github-typo-corpus.s3.amazonaws.com (github-typo-corpus.s3.amazonaws.com)... 52.216.137.116, 52.217.97.52, 52.217.196.121, ...\n",
      "Connecting to github-typo-corpus.s3.amazonaws.com (github-typo-corpus.s3.amazonaws.com)|52.216.137.116|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43769081 (42M) [application/x-gzip]\n",
      "Saving to: â€˜github-typo-corpus.v1.0.0.jsonl.gzâ€™\n",
      "\n",
      "github-typo-corpus. 100%[===================>]  41,74M   465KB/s    in 2m 18s  \n",
      "\n",
      "2023-03-14 17:55:23 (310 KB/s) - â€˜github-typo-corpus.v1.0.0.jsonl.gzâ€™ saved [43769081/43769081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\n",
    "!gzip -d github-typo-corpus.v1.0.0.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oiUTzkLNGr2q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 245909\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
    "\n",
    "dataset = []\n",
    "other_langs = set()\n",
    "\n",
    "with jsonlines.open(dataset_file) as reader:\n",
    "    for obj in reader:\n",
    "        for edit in obj['edits']:\n",
    "            if edit['src']['lang'] == 'eng' and edit['is_typo']:\n",
    "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
    "                if src.lower() != tgt.lower():\n",
    "                    dataset.append((src, tgt))\n",
    "                \n",
    "print(f\"Dataset size = {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjiKaUj0HKYo"
   },
   "source": [
    "Please, explore the dataset. You may see, that this is\n",
    "- mostly markdown\n",
    "- some common mistakes with do/does\n",
    "- some just refer to punctuation typos (which we do not consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gpkAqj6RHOr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \"\"\"Make am instance. =>         \"\"\"Make an instance.\n",
      "* travis: test agains Node.js 11 => * travis: test against Node.js 11\n",
      "The parser receive a string and returns an array inside a user-provided  => The parser receives a string and returns an array inside a user-provided \n",
      "CSV data is send through the `write` function and the resulted data is obtained => CSV data is sent through the `write` function and the resulting data is obtained\n",
      "One useful function part of the Stream API is `pipe` to interact between  => One useful function of the Stream API is `pipe` to interact between \n",
      "source to a `stream.Writable` object destination. This example available as  => source to a `stream.Writable` object destination. This example is available as \n",
      "`node samples/pipe.js` read the file, parse its content and transform it. => `node samples/pipe.js` and reads the file, parses its content and transforms it.\n",
      "Most of the generator is imported from its parent project [CSV][csv] in a effort  => Most of the generator is imported from its parent project [CSV][csv] in an effort \n",
      "*   `quote`             Optionnal character surrounding a field, one character only, defaults to double quotes.    => *   `quote`             Optional character surrounding a field, one character only, defaults to double quotes.   \n",
      "The parser receive a string and return an array inside a user-provided  => The parser receive a string and returns an array inside a user-provided \n"
     ]
    }
   ],
   "source": [
    "for pair in dataset[1010:1020]:\n",
    "    print(f\"{pair[0]} => {pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p0kp4G-HexP"
   },
   "source": [
    "Compare your implementation with Norvig's solution on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vy85_6oKHE3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 completed\n",
      "100 completed\n",
      "200 completed\n",
      "300 completed\n",
      "400 completed\n",
      "500 completed\n",
      "600 completed\n",
      "700 completed\n",
      "800 completed\n",
      "900 completed\n"
     ]
    }
   ],
   "source": [
    "limit = 1000\n",
    "counter = limit\n",
    "c_ngrams = 0\n",
    "c_norvig = 0\n",
    "for i, (src, target) in enumerate(dataset):\n",
    "    if i == limit:\n",
    "        break\n",
    "    # YOUR CODE HERE\n",
    "    corrected_sentence_norvig = norvig_corrector.correct_text(src)\n",
    "    corrected_sentence_n_gram = ngram_corrector.correct_text(src)\n",
    "    target = normalize(target)\n",
    "    if corrected_sentence_norvig == target:\n",
    "        c_norvig += 1\n",
    "    if corrected_sentence_n_gram == target:\n",
    "        c_ngrams += 1\n",
    "        \n",
    "    if i%100 == 0:\n",
    "        print(i, \"completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norvig solution: 262 out of 1000\n",
      "Ngrams solution: 95 out of 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Norvig solution: {c_norvig} out of {limit}\")\n",
    "print(f\"Ngrams solution: {c_ngrams} out of {limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ee870ab444af8a8689fba9fdb6a16993f9af4d6f8c51486b98fd7ee4129479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
